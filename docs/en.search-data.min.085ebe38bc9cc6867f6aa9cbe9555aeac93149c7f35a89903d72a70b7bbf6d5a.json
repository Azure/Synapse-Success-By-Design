[{"id":0,"href":"/Synapse-Success-By-Design/docs/home/","title":"Home","section":"Docs","content":"Azure Synapse Analytics #  Welcome to the Synapse Success By Design Repository. In this repository we have compiled the content that we have found helpful on our engagements with customers.\nAzure Synapse Analytics (CSE) Customer Sucess Engineering #  As part of the Synapse Engineering Group, we are called out to help and participate in complex projects covering any of the Synapse facets, ranging from a simple architecture review or customer question, up to a performance analysis and remediation proposal or even to act as an escalation with the Synapse Dev team on a critical situation blocking the project\nRepository #  This repository is intended to be a supplement to the official Azure Synapse documentation with additional specialized content and links to non-Microsoft generated content. We encourage you to review the official documentation also.\nMicrosoft Open Source Code of Conduct #  This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\nLegal Notices #  Microsoft and any contributors grant you a license to the Microsoft documentation and other content in this repository under the Creative Commons Attribution 4.0 International Public License, see the LICENSE file, and grant you a license to any code in the repository under the MIT License, see the LICENSE-CODE file.\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft’s general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\nPrivacy information can be found at https://privacy.microsoft.com/\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel or otherwise.\n"},{"id":1,"href":"/Synapse-Success-By-Design/docs/implementation-success/","title":"Implementation Success","section":"Docs","content":"Implementation Success #  Overview #  The Success By Design: Synapse Implementation Success method is designed to help guide you to a successful solution implementation that includes Azure Synapse Analytics as a component. This method is designed to complement your solution implementation project by adding suggested checks at strategic points during your project that can help assure a successful implementation of Azure Synapse Analytics. This method does not replace or change your chosen project management method (SCRUM, Agile, Waterfall), but will suggest validations to make while your project is in flight to improve the success of your overall deployment to a production environment.\nAzure Synapse is an enterprise analytics service that accelerates time to insight across data warehouses and big data systems. Azure Synapse brings together the best of SQL technologies used in enterprise data warehousing, Spark technologies used for big data, Pipelines for data integration and ETL/ELT, and deep integration with other Azure services such as Power BI, CosmosDB, and AzureML.\nThe Synapse Implementation Success method uses a strategic checkpoint approach to assessing and monitoring a project\u0026rsquo;s progress.\nThe goals of these checkpoints include:\n Proactive identification of possible issues and blockers Continuous validation of the solution\u0026rsquo;s fit to the use case Successful deployment to production Smooth operation and monitoring once in production  Synapse Implementation Success checkpoints are invoked at four key places within the Implementation Project:\n Project Planning Solution Development Pre Go-Live Post Go-Live  Synapse Implementation Success Strategic Checkpoints #  Here are the strategic checkpoints we recommend during these four project phases\nProject Planning #  Solution Evaluation #  The Solution in total will need to be evaluated with a focus on how it is making use of Azure Synapse as part of the architecture. An assessment will gather data that will identify the components of Azure Synapse that are being used, the interfaces each will have with other products, review the data sources, the data consumers, the personas, and use cases. This assessment will also gather data about the existing environment including detailed specifications from existing data warehouses, big data environments and integration and data consumption tooling. The assessment will identify which Synapse components will be implemented and thus what evaluations and check points should be made throughout the implementation effort. This assessment will also provide additional information to make validate design and implementation against requirements, constraints and assumptions.\n Perform an Assessment of the solution Identify the Azure Synapse Analytics solution components Execute Evaluations based upon the solution components  Evaluate Workspace Design Evaluate Data Integration Design Evaluate Dedicated SQL Pool Design Evaluate Serverless SQL Pool Design Evaluate Spark Pool Design   Review the results of these evaluations and respond accordingly  Project Plan Evaluation #  Evaluate the project plan as it relates to the Azure Synapse Analytics work that needs to be completed. This evaluation is not to make project plan changes but to identify steps that may be missing that could lead to blockers and impact the planned timeline. Once evaluated, the project plan may need to be modified and updated in response to the findings.\n Evaluate Project Plan at a high level. Evaluate project planning specific to the solution architecture components of Azure Synapse Analytics being implemented. Review the results and adjust accordingly.  Solution Development Environment Evaluation #  Evaluate the environment that is to be used to develop the solution. Having a controlled develop - test - production development environment incorporating automated deployment and source code control is essential to a successful and smooth solution development and the project success.\n Evaluate Solution Development Environment Design. Review the results and adjust accordingly.  Assess Team Readiness and Training #  Evaluate the project team with a focus their skill level and readiness to implement the solution incorporating Azure Synapse Analytics.\nIt will be important for the project’s success to have the correct skillsets. There are many and differing skillsets required for a successful implementation using Azure Synapse Analytics. Take time at this point to identify gaps and secure resources with the required skillset or take the time to complete training. This evaluation is critical at this stage. The lack of the proper skillset will impact both the timeline and the overall success of the solution.\n Evaluate the team\u0026rsquo;s Azure Synapse Analytics Skillset Secure additional skilled team members or skill-up team members  Solution Development #  Periodic Quality Checks #  Throughout the solution development checks should be made periodically to validate that the solution is being built with respect to recommended practices as they apply to the different components of Azure Synapse Analytics. Check that the project use cases will be satisfied, and the enterprise requirements are being met. For the purposes of this method, we are calling these periodic evaluations quality checks. Implement the following quality checks as appropriate to your solution.\n Quality Checks for Workspaces Quality Checks for Data Integration Quality Checks for dedicated SQL Pools Quality Checks for serverless SQL Pools Quality Checks for Spark Pools  Recommend Additional Skill Building #  As the project progresses identify if additional skills are needed. Take the time to get the additional skillset to improve the quality of the solution and avoid project delays and project timeline impacts.\nPre Go-Live #  Prior to deploying your solution to production, the following reviews are recommended to assess the preparedness of the solution and the organization for adoption and support of the solution on Azure Synapse Analytics. The Go-Live checklists for the specific Azure Synapse Analytics components will provide a final check of readiness to successfully move to production.\n Operational Readiness Review User Readiness and onboarding plan Review  Post Go-Live #  Following the deployment to production the following reviews are recommended to assure that the solution is operating as expected now that it is in production. Select the reviews appropriate to the solution\n Monitoring Review  Conclusion #  The Success By Design Synapse Implementation Success method is provided to help you with implementing a solution incorporating Azure Synapse Analytics. By utilizing this method during your Azure Synapse Analytics implementation project, you should be able to identify and address any issues early on and deploy to production a solution that benefits your business and delights your users.\n"},{"id":2,"href":"/Synapse-Success-By-Design/docs/poc_sucess/","title":"POC Success","section":"Docs","content":"POC Success #  Whether it is an enterprise data warehouse migration, a big data re-platforming, or a green field implementation; each project traditionally starts with a proof of concept.\nThis Proof of Concept playbook provides a high-level methodology for planning, preparing, and running an effective proof of concept project. An effective proof of concept validates the fact that certain concepts have the potential for real-world production application. The overall objective of a proof of concept is to validate potential solutions to technical problems, such as how systems can be integrated or how results can be achieved through a specific configuration.\nThese playbooks will help you to evaluate the use of Azure Synapse Analytics for the migration of an existing workload. It has been designed with the following readers in mind:\n Technical experts planning their own in-house Azure Synapse proof of concept project Business owners who will be part of the execution or evaluation of an Azure Synapse proof of concept project Anyone looking to learn more about data warehousing proof of concept projects  The playbook will deliver the following:\n Guidance on what makes an effective proof of concept Guidance on how to make valid comparisons between systems Guidance on the technical aspects of running an Azure Synapse proof of concept A road map to relevant technical content from Azure Synapse Guidance on how to evaluate proof of concept results to back business decisions Guidance on how to find additional help  To get started click the white paper below.\n   Dedicated SQL Pool Playbook Serverless SQL Pool Playbook Spark Pool Playbook    \nCombined PoC Playbook\n"},{"id":3,"href":"/Synapse-Success-By-Design/docs/video_series/","title":"Video Series","section":"Docs","content":"Video Series #  Here you will find videos covering various topics pertinent to Azure Synapse Analytics - dedicated SQL Pool, Serverless SQL Pool, and Spark Pool. These videos are designed to provide insight on various topics from how to conduct POCs to guidance on implementation when working with various areas of Synapse. Videos have been organized into series.\nProof Of Concept (POC) Series #     Dedicated SQL PoolDedicated SQL Deck Serverless SQL PoolServerless SQL Deck Spark PoolSpark Pool Deck    Guidance Series #     Spark Guidance OverviewSpark Pool Overview Deck Spark Pool Guidance DetailedSpark Pool Guidance DeckApache Spark for Azure Synapse Guidance Document    "},{"id":4,"href":"/Synapse-Success-By-Design/docs/guidance/","title":"Guidance","section":"Docs","content":"Guidance #  Here you will find documents and white papers covering various topics pertinent to Azure Synapse design, implementation and operations.\n   Azure Synapse Dedicated SQL pool Stream Ingestion Best Practices    "},{"id":5,"href":"/Synapse-Success-By-Design/docs/whats-new/","title":"Whats New","section":"Docs","content":"Whats new #  New on this site #      Date Details     09/13/2021 Azure Synapse Dedicated SQL pool Stream Ingestion Best Practices   07/20/2021 Apache Spark for Azure Synapse Guidance    New and exciting things in Azure Synapse Analytics #   20th June #     Service Improvement Details     Native Parquet reader in private preview Improved performance of Parquet external tables using new native technology in dedicated SQL pools Link to Article   SQLPackage.exe for Azure Synapse Analytics New features of SqlPackage.exe that are focused on Azure Synapse Analytics databases. Link to Article    For details about Synapse Dedicated SQL Pools visit the release notes.\nFor details about all of Azure Updates visit Azure Update site.\n"},{"id":6,"href":"/Synapse-Success-By-Design/docs/tools-utilities/","title":"Tools Utilities","section":"Docs","content":"Coming Soon Tools \u0026amp; Utilities #  "},{"id":7,"href":"/Synapse-Success-By-Design/docs/implementation-success/is_comingsoon/","title":"Is Comingsoon","section":"Implementation Success","content":"Coming Soon #  "},{"id":8,"href":"/Synapse-Success-By-Design/docs/implementation-success/post-golive/is_monitoringreview/","title":"Is Monitoring Review","section":"Implementation Success","content":"Monitoring Review #  Monitoring is a key part of the operationalising of any Azure Solution. Below is some guidance on reviewing and configuring the monitoring of your Azure Synapse environment. Key to this activity will be the identification of what needs to be monitored and who needs to be able to review the results of active monitoring. Using your solution requirements and other data collected during the assessment and during solution development, build a list of important behaviors and activities that need to be monitored in your live environment. As you build this list identify the user groups that will need access to this monitoring information and build the procedures to be followed in reaction to monitoring results. A multitude of activities can be monitored within Azure and we will focus here on monitoring the data and pipeline components of Azure Synapse Analytics.\nAzure Monitor #  Azure Monitor provides base-level infrastructure metrics, alerts, and logs for most Azure services. Azure diagnostic logs are emitted by a resource and provide rich, frequent data about the operation of that resource. Azure Synapse Analytics can write diagnostic logs in Azure Monitor.\nWorkspaces #  The following article explains how to monitor a Synapse Analytics workspace using Azure Monitor.\nSQL Pools (Dedicated) #  Monitoring a dedicated SQL Pool can be done in a number of ways;\n Azure Monitor Dynamic Management Views (DMV) Log Analytics  Azure Monitor #  How to use Azure Monitor to Monitor Azure Synapse SQL Pools.\nAlerting #  Alerts can send you an email or call a web hook when some metric (for example database size or CPU usage) reaches the threshold. This article shows you how to set up alerts for databases in Azure SQL Database and Azure Synapse Analytics using the Azure portal. https://docs.microsoft.com/en-us/azure/azure-sql/database/alerts-insights-configure-portal\nDynamic Management Views #  Using Dynamic Management Views (DMVs) to monitor your workload including investigating query execution in SQL pool.\nLog Analytics #  Log Analytics is a tool in the Azure Portal to edit and run log queries from data collected by Azure Monitor Logs and interactively analyze their results. You can use Log Analytics queries to retrieve records matching particular criteria, identify trends, analyze patterns, and provide a variety of insights into your data.\nSetup and configure Log Analytics for SQL Pool.\nLog Analyitcs Tutorial\nSQL Pools (Serverless) #  Monitoring SQL requests with Synapse Studio #  This article explains how to monitor your SQL requests, allowing you to keep an eye on the status of running requests and discover details of historical requests.\nSpark Pools #  This article explains how to monitor your Apache Spark applications, allowing you to keep an eye on the latest status, issues, and progress.\nIn this tutorial, you will learn how to enable the Synapse built-in Azure Log Analytics connector for collecting and sending the Apache Spark application metrics and logs to your Azure Log Analytics workspace. You can then leverage an Azure monitor workbook to visualize the metrics and logs.\nPipelines #  With Azure Synapse Analytics, you can create complex pipelines that can automate and integrate your data movement, data transformation, and compute activities within your solution. You can author and monitor these pipelines using Synapse Studio.\nThis article explains how to monitor your pipeline runs, which allows you to keep an eye on the latest status, issues, and progress of your pipelines.\nConclusion #  Over time what you choose to monitor and how you choose to monitor will change as will the importance of different processes and systems. Configuration of monitoring in Azure is not a one time activity and review of what is being monitored and the value of the data collected should be reevaluated periodically so that the most useful information is always available for review.\n"},{"id":9,"href":"/Synapse-Success-By-Design/docs/implementation-success/pre-golive/is_continueddevpreparednessreview/","title":"Is Continued Dev Preparedness Review","section":"Implementation Success","content":"#  Coming Soon Continued Dev Preparedness Review #  "},{"id":10,"href":"/Synapse-Success-By-Design/docs/implementation-success/pre-golive/is_operationalreadinessreview/","title":"Is Operational Readiness Review","section":"Implementation Success","content":"Operational Readiness Review #  Overview #  Once you build a Synapse Analytics solution and it is ready to deploy, it is important to ensure the operational readiness of that solution. Performing an Operational Readiness Review helps evaluate the solution for its preparedness to provide optimal services to its users. Organizations that invest time and resources in assessing the operational readiness of their solutions before launch have a much higher rate of satisfaction than those who don\u0026rsquo;t. It is also important to conduct an Operational Readiness Review periodically (once a year, for example) post deployment, to ensure there is no drift in operational expectations.\nProcess and Focus Areas #  Service Operational Goals #  Service Expectations: Document service expectations from customer\u0026rsquo;s point of view. Get a buy-in from the business on these service expectations. Make any modifications as necessary to meet business goals and objectives of the service.\nSLAs/OLAs: Service Level Objectives (SLAs) of each Azure service varies based on the service. For example, we guarantee that, at least 99.9% of the time client operations executed on a Synapse Analytics database will succeed. We have similar SLAs defined for each of the Azure services here. Ensure these SLAs match with your own business SLAs and document any gaps as needed. It is also important to define any Operational Level Agreements (OLAs) between different teams and ensure that these align with the SLAs.\nSolution Readiness #  Architecture Description: Describe/Document the architecture end to end, calling out critical functionalities of differnet components of the architecture and how they interact with each other.\nScalability: Describe/Document scalability aspects of your solution. This should include aspects like what is the effort involved in scaling and what is the impact of it on business, can it handle sudden surges of user activity etc. Azure Synapse Analytics provides functionality for scaling with minimal downtime.\nReliability: Identify and document any single points of failure in your solution, along with how to remediate when such failures occur. This should include the impact of such failures on dependent services and to minimize the impact.\nDependencies: List all dependent services on the solution and their impact.\nSecurity #  Authentication: Ensure AAD (Azure Active Directory) authentication is being used where possible. If non-AAD authentication is in place, ensure strong password mechanisms in place and passwords are being rotated at regular intervals. Check the link below for a Microsoft whitepaper on password guidance:\nPassword Guidance - Microsoft Research\nAlso, ensure monitoring is in place for suspicious actions related to user authentication. Azure Identity Protection is a feature that helps detecting potential vulnerabilities and suspicious actions related to identities.\nSecurity best practices for your Azure assets - Azure security | Microsoft Docs\nAccess Control: Ensure proper access controls are in place following the principle of least privileges. Use security features available with Azure services to properly tighten the security of your solution. For example, Azure Synapse Analytics provides granular security features such as RLS (Row-level Security), CLS (Column Level Security), DDM (Dynamic Data Masking) etc. to properly secure your data with absolute minimum privileges needed.\nAzure security baseline for Azure Synapse dedicated SQL pool (formerly SQL DW) | Microsoft Docs\nThreats: Ensure proper threat detection mechanisms in place to prevent, detect and respond to threats. Azure Security Center provides integrated security monitoring across your Azure subscriptions and help detect threats that might otherwise go unnoticed.\nhttps://docs.microsoft.com/en-us/azure/security-center/security-center-introduction\nAdopt a centralized Security Information and Event Management (SIEM) solution that meets your organizational security needed. Azure Sentinel is one such scalable, cloud-native SIEM solution that provides intelligent security analytics and threat intelligence via alert detection, threat visibility, proactive hunting and automated threat response.\nWhat is Azure Sentinel? | Microsoft Docs\nMonitoring #  Expectations: Set and document expectations on the monitoring readiness with your business. These expectations should include things like, how do you monitor end-to-end user experience, does the monitoring include single-user experience, how do you notify on poor user experience, what metrics from each service do you monitor etc. You should also include details on your pro-active health checks and any mechanisms in place to take automatic actions for reactive incidents (Ex: raising a ticket automatically).\nAzure Monitor provides base-level infrastructure metrics, alerts, and logs for most of the Azure services. Azure Monitor | Microsoft Azure\nMetrics: List all the important metrics you monitor for each service in your solution along with their acceptable thresholds. For example, the following are the important metrics to monitor for a Synapse Dedicated SQL Pool:\nDWULimit\nDWUUsed\nAdaptiveCacheHitPercent\nAdaptiveCacheUsedPercent\nLocalTempDBUsedPercent\nActiveQueries\nQueuedQueries\nCheck the link below for a more comprehensive list of metrics for each component of the Azure Synapse Analytics:\nHow to monitor Synapse Analytics using Azure Monitor - Azure Synapse Analytics | Microsoft Docs\nIn addition, utilize Azure Service Heath service to get notified on Azure service incidents and planned maintenance. Azure Service Health | Microsoft Azure\nNotifications: Ensure proper notifications are in-place to notify appropriate personnel when incidents occur. The incidents could be proactive such as certain metric exceeded a pre-set threshold or reactive such as a failure of a component/service.\nOverview of alerting and notification monitoring in Azure - Azure Monitor | Microsoft Docs\nHA/DR #  High Availability: Define and document RTO (Recovery Time Objective) and RPO (Recovery Point Objective) for your data. Each of the Azure services publish a set of guidelines and metrics on the expected High Availability of the service. Ensure these HA metrics maps to your business expectations. If not, customization may be necessary to meet your HA requirements. For example, Synapse dedicated SQL pool supports an eight-hour RPO with automatic restore points. If that RPO is not sufficient for your business, user-defined restore points may need to be setup with appropriate frequency to meet your RPO needs.\nBackup and restore - snapshots, geo-redundant - Azure Synapse Analytics | Microsoft Docs\nDisaster Recovery: #  Define and document a well-defined process for DR scenarios, such as failover process, communication mechanisms, escalation process, war room setup etc. Also document process for identifying the causes of outages and the steps to be taken to recover from the disasters. Utilize the built-in DR mechanisms available with Azure services for building your DR process. For example, Azure Synapse Analytics performs a standard geo-backup of SQL dedicated pools once every day to a paired data center. These geo-backups can be used to recover from a disaster at the primary location. Azure Data Lake storage can be configured to copy data to another Azure region that is hundreds of miles apart. In case of a disaster at primary location, a failover can be initiated to transform the secondary storage location into primary.\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance#unsupported-features-and-services\nConclusion #  Utilizing the guidance in this document ensures the operational readiness of your Azure Synapse Analytics solution.\n"},{"id":11,"href":"/Synapse-Success-By-Design/docs/implementation-success/pre-golive/is_userreadinessandonboardingplanreview/","title":"Is User Readinessand Onboarding Plan Review","section":"Implementation Success","content":"User Readiness and Onboarding #  Overview #  We certainly do our best to make our sophisticated cloud services simple and easy to maintain, operate and use. Authoring detailed documentation, training technical personnel, service/platform administrators and developers, etc. on how to use the system is just part of what needs to be done. We must extend this effort and make sure the end users of your solution understand it, are able to see how it fits their needs and how it can bring value. Educating end users is critical to the overall success of your solution.\nReview the use cases and personas identified during the assessment, project planning and solution development to assure that everyone will all be prepared to be successful on Day One. Evaluate the project\u0026rsquo;s plan for preparing all of the end users and support infrastructure of your solution.\nLook for an onboarding plan for these groups as they apply to your solution:\n  Big Data Analytics users\n  Structured Data Analytics users\n  Users of each and every one of your identified data consumption tools\n  Operations Support\n  Help Desk and User Support\n  Onboarding and Readiness #  Don\u0026rsquo;t expect the users to just figure out how to use Azure Synapse Analytics even if they have some experience on similar technologies. Have a plan to reach out to your user groups to ensure a smooth transition for them to the new environment.\nAs you reach out verify that:\n  users understand what Azure Synapse Analytics does and how it does it\n  users understand how to use the Azure Synapse service or platform that includes it\n  onboarding of users is a consistent and continuous process\n  users see and understand the value\n  Onboarding of users starts with explanatory sessions or technical workshops and giving them access to the platform will usually span several months depending on the complexity of the solution. This will set the right tone for future interactions with Azure Synapse platform and services.\nTake tracking steps and make sure the users are capable of successfully completing a set of core tasks that will be part of their day-to-day operations (or very close to). These tasks will be specific to different user groups, personas and use cases\nIdentify:\n  what core actions the users need to be able to perform\n  what steps the users must take to perform each action\n  Focus your efforts on points where the users struggle the most and provide straightforward instructions and processes. Be mindful of to how long it takes for the users to complete specific tasks. It is always a good idea to additionally request qualitative feedback to better track user readiness and onboarding experience.\nConclusion #  User and Support onboarding is of extreme importance \u0026ndash; providing even the best implementation of Azure Synapse platform to the users will likely fail if they do not have necessary understanding of its capabilities, the purpose of the features it provides them and how to perform operations that will lead to creation of business value.\n"},{"id":12,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/is_assessment/","title":"Is Assessment","section":"Implementation Success","content":"Assessment #  Overview #  This assessment exercise will provide you with the opportunity gather all the available information about the existing environment, environmental requirements, project requirements, constraints, timelines, pain points, etc. This information will be the basis of your later evaluations and checkpoint activities as it will be invaluable data to validate and compare against the project solution as it is being planned, designed, and developed. Take the time necessary to gather all this recommended information and have the necessary discussions with project stakeholders, business users, solution designers and subject matter experts (SME) of the existing solution and environment. The information gathered here will help you in evaluating that the designed solution is implementing the right components of Azure Synapse Analytics to support your solution and meet overall expectations and honoring corporate requirements.\nWorkload Assessment #  Environment #    Describe your existing analytical workload:\n  Type of workload (Datawarehouse, bigdata etc.)?\n  How is this workload helping the business? What are the Use case scenarios?\n  What is the business driver for this analytical platform and for potential migration?\n  Gather the details behind existing architecture, design, and implementation choices?\n  Gather details about all existing upstream and downstream dependent components and consumers?\n    Are you migrating existing data warehouse(s) (Netezza, Snowflake, Teradata, APS, SQL Server)?\n  Are you migrating a Big Data platform (Cloudera, Hortonworks etc.)?\n  Gather the architecture and dataflow diagram(s) for current analytical environment?\n  Where are the data sources for your planned analytical workloads located (Azure, Other cloud providers, On-Premises)?\n  Total size of dataset(s) (Historical and Incremental) being targeted to Azure. What is the current rate of growth of your dataset(s)? What is the projected rate of growth of your dataset(s) for the next 2-5 years?\n  Do you have an existing Data Lake? Gather as much detail as possible - file types (Parquet, csv etc.), sizes, security configuration, etc.?\n  Do you have semi-structured, unstructured data to process and analyze?\n  Describe the nature of the data processing (Batch, real time processing)?\n  Do you need interactive data exploration from relational data, data lake and other sources?\n  Do you need real time data analysis and exploration from operational data sources?\n  What are pain points and limitations in current environment?\n  What source control and Dev ops tool(s) are you using today?\n  Do you have a use case to build a hybrid (Cloud and on-prem) analytical solution, Cloud only or multi- cloud?\n  Gather data on existing cloud environment \u0026ndash; single-cloud provider or multi-cloud provider.\n  Gather plans on future cloud environment \u0026ndash; single cloud or multi cloud provider.\n  What are the RPO/RTO/HA/SLA requirements in the existing environment?\n  What are the RPO/RTO/HA/SLA requirements in the planned environment?\n  Analytical workload Personas #    Who are the different data personas (Data Scientist, Data Engineer, Data Analyst etc.)?\n  Describe the access control requirement on analytical platform for these personas.\n  Identify the platform owners who is responsible to provision compute and grant access.\n  Describe how different data personas are collaborating today?\n  Do you have multiple teams collaborating\\sharing the same analytical platform? What is the access control\\isolation requirements for each of these teams/data sets?\n  What are different client tools end user use to interact with Analytical platform?\n  ETL\\ELT, Transformation, and orchestration #    What tools are you using today for data ingestion, ETL/ELT?\n  Where do these tools exist in the existing environment (On prem, Cloud)?\n  What is your current data load/update requirements (real-time/micro batch/hourly/daily/weekly/monthly)?\n  Describe the transformation requirements for each layer (Bigdata [data lake], Datawarehouse etc.)?\n  What is the current programming approach(es) to transforming the data (No code, low code, programming like SQL, Python, Scala, C#, etc.)?\n  What is the preferred planned programming approach to transform the data (No code, low code, programming like SQL, Python, Scala, C#, etc.)?\n  What tools are currently in use for data orchestration to automate the data driven process?\n  Where are the data sources for your existing ETL located (Azure, Other cloud provider, On-Premises)?\n  What are the existing data consumptions tools (Reporting, BI tools, open-source tools) that require integration with analytical platform?\n  What are the planned data consumptions tools (Reporting, BI tools, open-source tools) that will require integration with analytical platform?\n  Network and security #    What regulatory requirements do you have for your data?\n  If your data contains PII\\PCI\\HIPPA data, has your security group certified Azure for this data? If so, which services in Azure?\n  Describe your user authorization and authentication requirements?\n  Are there security issues which could limit the access to data during implementation?\n  Is there test data available to be used during implementation development and testing?\n  Describe the organizational network security requirements on the Analytical compute and storage (Private network, public network, firewall restriction etc.)?\n  Describe the network security requirements for client tools to access analytical compute and storage? (Through peered network, private end point etc.)\n  Describe the current network setup between on prem and Azure (Express route, site to site etc.)?\n  Use below checklists and add\\modify as per your needs:\n   Data Protection\n   Data in Transit\n  Data Encryption at Rest (Service \u0026amp; BYOK)\n  Data Discovery and Classification\n   Access Control\n   Object Level Security\n  Row Level Security\n  Column Level Security\n  Dynamic Data Masking\n   Authentication\n   SQL Login\n  Azure Active Directory\n  Multi-Factor Authentication\n   Network Security\n   Virtual Networks\n  Firewall\n  Azure ExpressRoute\n   Threat Protection\n   Thread Detection\n  Auditing\n  Vulnerability Assessment\n  Azure Environment #    Are you currently using Azure? For Production?\n  If you are using Azure, which services are you using?\n  If you are using Azure, which regions are you using?\n  Do you have an Express Route in place? What is its bandwidth?\n  Do you have budget approval to provision required services in Azure?\n  How do you provision and manage resource today? (ARM, Terraform...)\n  Is your key team familiar with Synapse analytics.? Any training required?\n  Data Consumption and other Tools and services #    Describe how and what tools you currently use to perform activities like ingest, explore, prepare, and visualize the data. Identify what tools you plan to use to perform activities like ingest, explore, prepare, and visualize the data.\n  What applications planned to interact with analytical platform.? (Ex: Tools like PBI, Qlik, etc.).\n  Identify all data consumers\n  Identify data exports and data sharing scenarios\n  Assessment by Synapse analytics Services #  This section covers assessment aligned with services within Azure Synapse analytics. Synapse has the following components for compute and data movement:\nSynapse SQL\nSynapse SQL is a distributed query system for T-SQL that enables data warehousing and data virtualization scenarios and extends T-SQL to address streaming and machine learning scenarios.\nSynapse SQL offers both serverless and dedicated resource models.\nServerless SQL Pool\nServerless SQL pool is a distributed data processing system, built for large-scale data, and computational functions. There\u0026rsquo;s no infrastructure to setup or clusters to maintain. Best for unplanned or burst workloads, use the always available, serverless SQL endpoint. The recommended scenarios include quick data exploration on files directly on Data Lake, Logical date warehouse, Data Transformation of raw data.\nDedicated SQL Pool\nRepresents a collection of analytic resources that are provisioned when using Synapse SQL. The size of a dedicated SQL pool (formerly SQL DW) is determined by Data Warehousing Units (DWU). For a full management capability of a data warehouse with predictable and high performance for continuous workloads, create dedicated SQL pools to reserve processing power for data stored in SQL tables. Apache Spark\nApache Spark for Azure Synapse deeply and seamlessly integrates Apache Spark--the most popular open-source big data engine used for data preparation, data engineering, ETL, and machine learning.\nData Integration Pipelines\nAzure Synapse contains the same Data Integration engine and experiences as Azure Data Factory, allowing you to create rich at-scale ETL pipelines without leaving Azure Synapse Analytics.\nAdditional Assessment information focused on Synapse SQL #  Assessment questions to help determine the best SQL Pool Type (dedicated or serverless) #    Do you want to build a traditional relational data warehouse by reserving processing power for data stored in SQL tables?\n  Do use cases demand predictable performance?\n  Do you want to a build logical warehouse on top of a Data Lake?\n  Do you want to query data directly from a data lake?\n  Do you want to explore data from?\n  Additional Assessment information focused on dedicated SQL Pool #  Platform\n  What is the current DW platform (SQL Server, Netezza, Teradata, Greenplum, greenfield, etc.)?\n  If this is migration workload, provide make and Model of your Appliance for each Environment? (CPUs, GPUs, memory).\n  If this is appliance migration, when was the Hardware Purchased?\n  If this is appliance migration, Has the appliance been depreciated 100%? If not, when will depreciation end? Now how much Capex is still left.\n  Any a Hardware/Network Architecture diagram...?\n  Where are the data sources for your planned Azure Data Warehouse located (Azure, Other cloud provider, On-Premises)?\n  What are the data hosting platforms of the data sources for your data warehouse - (DB2, Oracle, SQL Server, Azure SQL, Azure Blob storage, AWS, Hadoop, etc.)?\n  Are any of the data sources data warehouses? Which ones?\n  Identify all ETL/ELT/Data Loading scenarios - Batch windows, streaming, near real-time. Identify existing SLAs for each and document the expected SLAs in the new environment.\n  What is the current data warehouse sizes?\n  Rate of growth of dataset being targeted for Synapse Dedicated sql pool.\n  Describe the environments you are using today (Dev/QA/Prod/DR).\n  Which tools are currently in place for data movement (SSIS, Robocopy, Informatica, SFTP, ADF)?\n  Are you planning on loading Realtime or near-real time data?\n  Database(s)\n  Number of objects in each data warehouse - schemas, tables, views, stored procedures, functions. Overall complexity \u0026ndash; Star or snowflake or other data structure.\n  Largest tables in terms of size\\number of records?\n  Widest table in terms of number of columns?\n  Is there already a data model designed for your data warehouse? Is it a Kimball, Inmon or Star Schema design?\n  Are Slowly Changing Dimensions (SCD) in use? If so, which types (SCDI, SCDII, etc.).\n  Will a Semantic Layer be implemented using Relational Data Marts or Analysis Services (Tabular/Multi-Dimensional) or another product/technique?\n  What is the HA/RPO/RTO/Data archiving requirements?\n  What is the region replication requirement?\n  Workload Characteristics\n  What is the estimated number of concurrent users/jobs accessing the data warehouse during peak hours?\n  What is the estimated number of concurrent users/jobs accessing the data warehouse during off peak hours (normal hours)?\n  Are there a period where there will be zero users or jobs?\n  What are your query execution performance expectations for interactive queries?\n  What are your data load performance expectations for daily/weekly/monthly data loads/updates? (ELT/ETL loading)\n  What are your query execution expectations for reporting/analytical queries?\n  How complex will be the most the commonly executed queries?\n  What percentage of your total dataset size is your Active dataset?\n  Approximately what percentage of the workload is anticipated for loading/updating, batch processing/reporting, interactive query, and analytical processing?\n  Identify the data consuming pattern and platforms:\n  Current and planned reporting method and tools.\n  Which application/analytical tools will be accessing the data warehouse?\n  Number of concurrent queries?\n  Average number of active queries at any point?\n  Nature of data access (Interactive, ad hoc, exports etc.)\n  Data personas and complete description of their interaction with the data.\n  Maximum number of concurrent connections\n    Query Performance SLA pattern by:\n  dashboard users\n  batch reporting\n  data mining users\n  ETL process\n    What are the security requirements for the existing environment and for the new environment (row level security, column security requirements, access control, encryption etc.)?\n  Do you have use cases to integrate Machine learning model scoring with SQL?\n  Additional Assessment information focused on serverless SQL Pool #    Do you have use cases to discover and explore data from a Data Lake using familiar T-SQL?\n  Do you have use cases to build a logical warehouse on top of a data lake?\n  Identify if there are use cases to transform data in Data Lake without moving data out from Data Lake.\n  Is your data already in Azure Data Lake Storage or Azure Blob storage?\n  If data is already in Azure Data Lake Storage, do you have good partition strategy in the data lake?\n  Do you have operational data in Cosmos dB? Do you have use cases for Realtime analytics on Cosmos Db without impacting transactions?\n  Identify the file types in the Data Lake.\n  Identify the query performance SLA. Does your use case demand predictable performance and cost?\n  Do you have unplanned or bursty SQL analytical workloads?\n  Identify the data consuming pattern and platforms:\n  Current and planned reporting method and tools.\n  Which application/analytical tools will be accessing the Serverless sql pool?\n  Number of active queries.?\n  Nature of data access (Interactive, ad hoc, exports etc.)\n  Data personas?\n  Concurrent connections?\n  Query complexity?\n    What are the security requirements (Access control, encryption etc.)?\n  What is the required T-SQL functionality (Stored proc, function etc.)?\n  Identify number of queries hitting the Serverless SQL pool and result set size from each query.\n  Synapse Serverless SQL Pool has three major use cases:\n  Basic discovery and exploration - Quickly reason about the data in various formats (Parquet, CSV, JSON) in your data lake, so you can plan how to extract insights from it.\n  Logical data warehouse \u0026ndash; Provide a relational abstraction on top of raw or disparate data without relocating and transforming data, allowing always up-to-date view of your data.\n  Data transformation - Simple, scalable, and performant way to transform data in the lake using T-SQL, so it can be fed to BI and other tools or loaded into a relational data store (Synapse SQL databases, Azure SQL Database, etc.).\n  For complete guide on Synapse Serverless Sql Pool follow below learning path:\n[Build data analytics solutions using Azure Synapse serverless SQL pools\n Learn | Microsoft Docs](https://docs.microsoft.com/en-us/learn/paths/build-data-analytics-solutions-using-azure-synapse-serverless-sql-pools/)     Different professional roles can benefit from serverless SQL pool:\n  Data Engineers can explore the lake, transform, and prepare data using this service, and simplify their data transformation pipelines. For more information, check this tutorial.\n  Data Scientists can quickly reason about the contents and structure of the data in the lake, thanks to features such as OPENROWSET and automatic schema inference.\n  Data Analysts can explore data and Spark external tables created by Data Scientists or Data Engineers using familiar T-SQL language or their favorite tools, which can connect to serverless SQL pool.\n  BI Professionals can quickly create Power BI reports on top of data in the lake and Spark tables.\n  T-SQL Feature Comparison between SQL Pool Dedicated pool and Serverless Pool:\nT-SQL feature differences in Synapse SQL - Azure Synapse Analytics | Microsoft Docs\nAdditional Assessment information focused on Spark Pool #    Identify the workloads that require data engineering and/or data preparation.\n  Clearly define the types of transformations.\n  Identify if you have unstructured data to process.\n  If you are migrating from an existing Spark\\Hadoop workload:\n  What is the existing big data platform (Cloudera, Hortonworks, Cloud services etc.)?\n  If it is a migration from on-prem, has hardware\\License been Expired or depreciated? If not, when will depreciation\\License will end?\n  Existing cluster type, Client tools?\n  Required Libraries, Spark versions?\n  Is it a Hadoop migration to Spark?\n  Current\\preferred programming languages.\n  Type of workload (Big data, ML, etc.)?\n  Existing and planned client tools (IDE\u0026rsquo;s) and reporting platforms?\n  Security requirements?\n  Current pain points and limitations?\n    Do you plan to use or are currently using Delta Lake?\n  How do you manage package today?\n  Identify the required compute cluster types?\n  Identify if cluster customization is required?\n  Spark pools in Azure Synapse Analytics enable the following key scenarios:\n  Data Engineering/Data Preparation - Apache Spark includes many language features to support preparation and processing of large volumes of data so that it can be made more valuable and then consumed by other services within Azure Synapse Analytics. This is enabled through multiple languages (C#, Scala, PySpark, Spark SQL) and supplied libraries for processing and connectivity.\n  Machine Learning - Apache Spark comes with MLlib, a machine learning library built on top of Spark that you can use from a Spark pool in Azure Synapse Analytics. Spark pools in Azure Synapse Analytics also include Anaconda, a Python distribution with a variety of packages for data science including machine learning. In addition, Apache Spark on Synapse provides pre-installed libraries for Microsoft Machine Learning, MML, a fault-tolerant, elastic, and RESTful machine learning framework. When combined with built-in support for notebooks, you have an environment for creating machine learning applications.\n  To get started with Synapse spark refer below guided learning:\nWhat is Apache Spark - Azure Synapse Analytics | Microsoft Docs\nPerform data engineering with Azure Synapse Apache Spark Pools - Learn | Microsoft Docs\nAzure Spark supports Delta Lake, for complete guide follow the below guide:\nOverview of how to use Linux Foundation Delta Lake in Apache Spark for Azure Synapse Analytics - Azure Synapse Analytics | Microsoft Docs\nConclusion #  An effective assessment will help you to understand the existing analytical environment with current data analytical use cases and the planned analytical environment including future data analytics requirements.\nThis assessment is a guide help evaluate the solution design and make informed technology recommendations for the best services to implement within the Azure Synapse.\nBy the end of the assessment, you should have clear view of the existing platform, Analytical use cases, pain points and expectations for the new platform.\n"},{"id":13,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/is_referencearchitectures/","title":"Is Reference Architectures","section":"Implementation Success","content":"#  Coming Soon Reference Architectures #  "},{"id":14,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/project-plan-evaluation/is_evaluateprojectplanhighlevel/","title":"Is Evaluate Project Plan High Level","section":"Implementation Success","content":"High-Level Project Plan Evaluation #  Overview #  In the project\u0026rsquo;s lifecycle, the most important and extensive planning is performed before implementation. This article will provide a high-level QUICK review of the project plan to make sure it contains critical artifacts and information for a successful project.\nBelow is the checklist of items that must be defined and approved before the project is initiated.\nFollowing this high-level project plan review a more detailed review will be conducted focusing on the specific Azure Synapse components identified during the Assessment.\nEvaluate the Project Plan #  Verify that in addition to detailed project tasks the following items have been defined in the project plan and that they align with the information gathered during the Assessment.\nThe Project Plan\n  Core Resource Team \u0026ndash; core group of key people, each of whom offers expertise crucial to the project.\n  Scope - this document how the project scope will be defined, verified, measured, and how the work breakdown will be created and defined.\n  Schedule \u0026ndash; this defines the time duration required to complete the project.\n  Cost \u0026ndash; costs for resources, internal and external, infrastructure/hardware/software\n   As the work breakdown is defined and assigned, we also want to make sure we have the following artifacts defined in addition to the project plan:\n  Migration plan \u0026ndash; the plan to migrate from your current legacy system to Synapse. Tasks for executing migration should be incorporated within the project plan and timeline.\n  Success Criteria \u0026ndash; the critical success criteria for stakeholders/project sponsor, go/no go criteria.\n  Quality Assurance \u0026ndash; defining code reviews, development/staging/production promotion approval process.\n  Test plan \u0026ndash; defining test cases, success criteria for unit, integration, user testing and metrics to validate the deliverables. Tasks for developing and executing the test plans should be incorporated within the project plan and timeline.\n  Evaluate Project Plan Detailed Tasks #  The list of items in the above Check Lists provides high-level artifacts to ensure that this important information is well documented, defined, and approved by all parties. Once you have gathered these documents, the next step is to drill down into each component of the project plan.\nPlease see the following documents for more detailed information on Evaluation of the Project Plan with respect to the specific Azure Synapse area:\n  Evaluate Project Plan for Workspace implementation\n  Evaluate Project Plan for Data Integration implementation\n  Evaluate Project Plan for dedicated SQL Pool implementation\n  Evaluate Project Plan for serverless SQL Pool implementation\n  Evaluate Project Plan for Spark Pool implementation\n  Conclusion #  Upon completion of this review of the Project Plan you will have verified that all the key project artifacts have been defined and that all the key project tasks for successfully implementing the solution using Azure Synapse have been scheduled and properly resourced. The project plan will have been reviewed for key success factors by comparing the plan with information gathered during the Assessment to validate against known constraints and assumptions.\n"},{"id":15,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/project-plan-evaluation/is_solutiondevelopmentenvironmentdesignevaluation/","title":"Is Solutiondevelopmentenvironmentdesignevaluation","section":"Implementation Success","content":"Solution Development Environment Design Review #  Overview #  Solution Development and the Environment within which this will be performed is key to your project\u0026rsquo;s success. Regardless of the project methodology selected (waterfall, Agile, scrum, etc.) multiple environments \u0026ndash; development \u0026ndash; test/User acceptance \u0026ndash; Production with clear processes for promoting changes between environments will be required.\nSetting up a modern data warehouse environment for both production and pre-production use can be complex. Keep in mind that one of the key design decisions is automation - it helps increase productivity while minimizing the risk of errors. On top of it we must be able to support future agile development, including the addition of new workloads like data science, RT, etc. During this design review we will review the solution development environment design that will support your solution not only for this project but for ongoing support and development of this solution. We will review the options and recommendations for solution development environments and compare this guidance with the design. When completed we will have reviewed the key components for development and support of this solution that will be instrumental to this project\u0026rsquo;s solution development phase and ongoing support and development.\n Solution Development Environment Design #  In the scope of solution development environment design scenarios there are many common requirements. The environment design should include the Production environment which will host the production solution and at least one non-production environment \u0026ndash; most environments contain two non-production environments: one for development and another for QA/testing/User Acceptance(UAT). Typically, they are hosted in separate Azure Subscriptions, a production subscription, and a non-prod subscription. This provides a very clear security boundary and delineation between non-prod and production.\nFor the purposes of this review, we will identify three environments:\nDevelopment \u0026ndash; this is the environment within which your data and analytics solutions are going to be built. Check if this environment must be able to spawn further and provide sandboxes for developers who need to make and test their changes in isolation while a shared Development environment will host integrated changes from the entire development team.\nTest/QA/UAT \u0026ndash; this is the production-like environment for deployments to be tested before going to production.\nProduction - is the final production environment.\n Synapse Workspaces #  For each Synapse Workspace in the solution the Environment should include a production Synapse Workspace and at least one non-production Synapse Workspace, Dev and QA/Test/UAT.\nAll pools and artifacts within the workspace (Spark Pools, Databases, SQL Pools, etc.) should be named the same across environments to ease the promotion of your workspace and its pools and artifacts to another environment, there are two parts.\n  Use an Azure Resource Manager template (ARM template) to create or update the workspace resources (pools and workspace)\n  Migrate artifacts (SQL scripts, notebook, Spark job definition, pipelines, datasets, data flows, and so on) with Azure Synapse Analytics CI/CD tools in Azure DevOps\n   Azure DevOps / GitHub #  Review that integration with Azure DevOps or GitHub is properly configured: there should be a repeatable process that moves changes across dev, test and production environments.\n Sensitive configuration data should always be stored securely in Azure Key Vault(s): maintain a central, secure location for sensitive configuration data such as database connection strings that can be access by the appropriate services within the specific environment.\n Conclusion #  Solution Development Environment design will improve the quality of your solution and the resilience of your solution to unintended or un-tested changes. The design of the solution development environment should support your project methodology and protect your solution and your data. This review will have validated the design of your solution development environment.\n"},{"id":16,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/solution-evaluation/is_dataintegrationdesignevaluation/","title":"Is Data Integration Design Evaluation","section":"Implementation Success","content":"Data Integration Design Evaluation #  Overview #  Azure Synapse Analytics is a complete analytics platform where business can choose one of three analytics runtimes (Apache Spark, Serverless SQL or Dedicated SQL Pool) for converting raw data into meaningful insights. It has a robust built-in data integration module called Synapse Pipeline for building effective, scalable and secured data pipelines.\nIn this guide we will evaluate the design of the Data Integration components of your project. Upon conclusion we will be able to determine if Synapse Pipelines are the best fit for your data integration use cases and we will have checked that all the key aspects of the design have been considered. Time invested in evaluating the design prior to solution development will help to eliminate unexpected design changes that may impact your project timeline or cost.\nDesign Review #  Fit-Gap Analysis #  You need to do a thorough fit-gap analysis of the data integration strategy. If the design has chosen Synapse Pipelines as one of the data integration tools to be used in the solution, review the following points to make sure Synapse Pipeline is the best tool for data integrations and orchestration. Some of the key aspects identified below need to be assessed in your environment. If another data integration tool or tools have been specified in the solution design the following points should still be reviewed to validate that these key design points have been considered and the chosen tool will support you solution\u0026rsquo;s needs. This information should have been captured during your assessment performed earlier in this method.\n  Review your data sources and destinations (targets)\n  Validate source and destination store are supported data stores\n  If not supported, check if you can leverage the extensible options of pipeline\n    Review the triggering points of your data integration and the frequency\n  Synapse pipeline supports schedule, tumbling window and storage event triggers\n  Validate minimum recurrence interval and supported storage events against your requirement\n    Review the modes of data integration required\n  Scheduled, periodic and triggered batch processing can be effectively designed in Synapse pipelines\n  To support Change Data Capture capabilities, leverage third party products or create a custom solution\n  To support real-time streaming, leverage Event Hub/Kafka/IoT Hub\n  To support Lift-and-Shift of SSIS packages, leverage Azure Data Factory SSIS-IR\n    Review the compute design. Does the compute required for the pipelines need to be serverless or provisioned?\n  Synapse pipeline supports both modes where integration runtime can be serverless or self-hosted on a windows machine\n  Validate ports and firewall and proxy setting in case of self-hosted IR (provisioned)\n    Review the security requirements, networking, and firewall configuration of the environment and compare to the security, networking and firewall configuration design.\n  Review how the data sources are secured and networked\n  Review how the target data stores are secured and networked\n  Synapse pipeline has different data access strategies to provide a secured way to connect data stores via private endpoints/VNET/Firewall/Internet\n  Use Azure key-vault to store credentials whenever applicable\n  Leverage Azure Data factory for customer managed key (CMK) encryption or encryption credentials and store in self-hosted IR\n    Review the design for ongoing monitoring of the data integration components\n  Architecture Considerations #  As you review the data integration design consider the following recommendations and guidelines to ensure that the data integration components of your solution will provide ongoing operational excellence, performance efficiency, reliability and security .\nOperational Excellence #  Environment: When planning for environments, segregate it by dev/test, UAT, and Production. Having a clean and working lower environment (with working data connections and pipelines) makes development and support streamlined and smooth.\nLeverage the Folder organizational options to organize your pipelines and datasets by business/ETL jobs for better maintainability. Use Annotations to tag your pipelines which later can be used in monitoring.\nLeverage parameters and Iterations/Conditionals activities to create re-usable pipelines.\nMonitoring and Alerting: Synapse workspace includes Monitor hub which has rich monitoring information of each and every pipeline run. It also integrates with Log Analytics for further log analysis and alerting. These features should be implemented to provide proactive error notifications. Leverage \u0026ldquo;Failure\u0026rdquo; branch for implementing user-defined error handling.\nAutomated Deployment and Testing: Synapse pipeline is built into Synapse workspace, so you can take advantage of Workspace automation/deployment. Minimize manual activities while creating Synapse workspaces across environments by using ARM templates. Integrate Synapse workspaces with Azure DevOps to build code versioning and automated publication.\nPerformance Efficiency #  There are multiple factors that impact performance of pipeline execution. Some of the key points are.\n  Follow performance guide and optimization features when working with the Copy activity\n  Choose optimized connectors for data transfesr instead of generic ones. For example, PolyBase instead of bulk insert when moving data from ADLS Gen2 to Dedicated SQL pool\n  When creating a new Azure IR, pick the region as Auto Resolve or select the same region as the data stores\n  For Self-hosted IR, choose the Azure VM size based on the integration requirements\n  Choose a stable network connection like Azure ExpressRoute for fast and consistent bandwidth\n  Reliability #  Availability: When you are executing a pipeline using Azure IR, it is serverless in nature and provides resiliency out of the box. There is little for customers to manage. But when a pipeline is running in a self-hosted IR, it is recommended to run using a HA configuration using Azure VMs. This will ensure integration pipelines are not broken even when a VM goes down. Secondly, it is recommended to have Azure ExpressRoute for a fast and reliable network connection between on-premises and Azure.\nSecurity #  A secured data platform is one of the key requirements of every organization. A lot of thought needs to be given to overall security of the platform rather than individual components. Here are some security guidelines for Azure Synapse Pipeline solutions.\n  Secure data movement to cloud using Azure Synapse private endpoints\n  Use AAD/MSI for authentication and Azure RBAC and Synapse RBAC for authorization\n  Store credentials, secrets and keys in Azure Key Vault rather than in pipeline (link)\n  Integrate on-premises via Azure ExpressRoute or VPN over private endpoints.\n  Enable Secure input/output in pipeline activities for parameters storing secrets/passwords\n  Conclusion #  Upon completion of this review of the data integration design of your solution you should know if the data integration tool(s) selected for your implementation will meet all the requirements of your organization and you will have taken the time to consider and validate that many important guidelines and recommendations have been reviewed and applied to your data integration design. Prior to solution development is the best time to make design modifications to better meet your solution needs and will improve the overall success of your solution and your project.\n"},{"id":17,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/solution-evaluation/is_dedicatedsqlpooldesignevaluation/","title":"Is Dedicated SQL Pool Design Evaluation","section":"Implementation Success","content":"Dedicated SQL Pool Design Evaluation #  Overview #  The purpose of this evaluation is to validate the solution design\u0026rsquo;s dedicated SQL Pool components and to identify early on any design issues and, to validate the design meets common guidelines. By evaluating the design before solution development begins blockers and unexpected design changes will be avoided and protect the project\u0026rsquo;s timeline and budget.\nDedicated SQL pool refers to the enterprise relational data warehousing features that are available in Azure Synapse Analytics.\nSynapse SQL leverages a scale out architecture to distribute computational processing of data across multiple nodes. Compute is separate from storage, which enables you to scale compute independently of the data in your system.\nReview: Dedicated SQL pool architecture - Azure Synapse Analytics | Microsoft Docs\nAssessment Analysis #  During your initial assessment you collected information about how your existing system was deployed and the details of structures that were implemented.\nFrom the assessment review, you will be able to understand what the gaps are between what is implemented and what should be in place to get optimal performance. For example, the impact of having round robin table vs Hash distributed tables or the performance benefits of correctly using replicated tables.\nReviewing the Target Architecture #  For the successful deployment of a dedicated SQL Pool, it is important to have an architecture well aligned with the business requirements to make sure that the system responds to all your business needs.\nThere are several architectural suggestions for dedicated SQL Pools in the Microsoft documentation and if necessary you can review the Azure Data Architecture Guide - Azure Architecture Center | Microsoft Docs\nMigration #  A migration project for synapse is very similar to any other Database migration. You will need to take in consideration that there might be differences between the original system and the Synapse.\nMake sure that you have a clear migration path established for:\n  DDL/DML/Security objects/etc.\n  Data migration (export from source, transit to cloud)\n  Initial Data load to Synapse\n  Login/Users\n  Data Access control (for ex. Row-Level Security)\n  Time plan to execute.\n  You can also review the Azure Synapse Analytics: Migration guide - Azure Synapse Analytics | Microsoft Docs\nFeature Gaps #  From the information gathered during your assessment and the solution design verify that any unsupported features from the existing environment that will be required in the solution have been identified and that a solution for supportability has been designed or that there is time built into the project plan to further identify these unsupported features and develop solutions and workarounds.\nSome examples of unsupported features in dedicated SQL Pools are:\n  Identify unsupported data types (ex. Xml, arrays, spatial)\n  Identify unsupported features (ex: Cursors)\n  Dedicated SQL Pool Testing #  Just like any other project, your project should include the necessary set of tests to make sure that your dedicated SQL Pool responds as required to the needs of your business. It is critical to test at least for data quality, data integration, security, and performance.\nConclusion #  Taking the time to evaluate the design against the information gathered during the assessment and the capabilities of a dedicated SQL Pool will help assure that the best design for your solution will be implemented, and it will reduce the number of unexpected development or implementation issues that may be encountered during solution development.\n"},{"id":18,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/solution-evaluation/is_serverlesssqlpooldesignevaluation/","title":"Is Serverless SQL Pool Design Evaluation","section":"Implementation Success","content":"Serverless SQL Pool Design Evaluation #  Overview #  Separation of storage and compute as a design principle for modern data and analytical platforms and services has been a trend and frequently used pattern, it provides cost savings and more flexibility in on-demand scaling of your storage and compute independently. Synapse SQL Serverless further extends this pattern and adds an important capability to query your data directly from your data lake storage and not to worry about compute management while leveraging self-service types of workloads.\nThe purpose of this evaluation is to provide guidance for architectures that include Synapse SQL Serverless pools and to try to reduce blockers frequently found during the course of the implementation of modern data architectures based on Azure Synapse or later in the production stages.\nFit Gap Analysis #  When planning to implement SQL Serverless pools within Azure Synapse Analytics, you first need to ensure Serverless Pools are the best fit for your workloads. These are the items to consider for utilizing Apache Spark in Synapse:\nOperational Excellence #    Solution Development Environment: Within this method there is a review of the solution development environment. Identify how the environments are designed to support solution development (dev/test/prod). Most commonly you will find a prod and non-prod environment with Production being placed in the prod environment and Dev and Test placed in a non-prod environment. You should find the Synapse Workspaces in the solution to exist in all of the environments (dev/test/prod). In most cases, you are obliged to segregate your production and dev/test users and workloads.\n  Synapse Workspace Design: Within this method there is a review of the Synapse Workspace design. Identify how the Workspace(s) have been designed for your solution. Become familiar with the design and know if the solution will utilize a single workspace or if multiple workspaces are part of the solution. Know why a single or multiple workspace design was chosen. A multi-workspace design is usually chosen to support strict security boundaries.\n  Deployment: SQL Serverless per se does not require any special deployment actions and you will have that service available on-demand with every Synapse workspace. Check regional proximity of the service and that of the Data Lake Storage Gen2 you access with SQL Serverless.\n  Monitoring: check if built-in monitoring is sufficient and if any external services need to put in place to store historical log data, in order to be able to analyze changes in performance or allow you to define alerting or triggered actions for specific situations.\n  Performance Efficiency #  Unlike traditional database engines, SQL Serverless does not rely on its own optimized storage layer and for that reason its performance is strongly dependent on how data is organized in Azure Data Lake Storage.\n  Data Ingestion: Review how data is stored in Data Lake Storage - the file sizes, number of files, and folder structure all have an impact on performance. Keep in mind that while some file sizes might work for SQL Serverless they may impose issues for efficient processing or consumption by other engines and applications. You will need to evaluate the data storage design and validate it against all of the data consumers plus SQL Serverless and any other data tools that are part of your solution.\n  Data Placement: Evaluate if your design has unified and defined common patterns for data placement. Make sure that directory branching can support your security requirements. There are a few common patterns that help you keep your time series data organized. Whichever your choice is, make sure that it also works with other engines and workloads, validate if it can help partition auto-discovery for Spark applications and external tables.\n  Data Formats: SQL Serverless in most cases will offer the best performance and better compatibility feature-wise using a Parquet format. Verify your performance and compatibility requirements, because while Parquet improves performance through better compression and reduction of IO by reading only required columns needed for the analysis, it requires additional compute and some source systems do not natively support Parquet for export output format which would lead for additional transformation steps in your pipelines and/or dependencies in your overall architecture.\n  Exploration: Every industry is different, but in many cases, there are certain data access patterns that will be common and found in the most frequent queries in your workloads - filtering and aggregations by dates, categories, or geographies \u0026ndash; these are core, common patterns across many businesses. Identify from your assessment your most common filtering criteria, relate these to how much data is being read/discarded by top used queries and validate if the information on Data Lake is organized to favor your exploration performance expectations.\n  From the queries identified in your design and in your assessment see if you effectively eliminate unnecessary partitions in your OPENROWSET path parameter or, in case of external tables, if additional indexing is required.\n  Reliability #    Availability: Validate the requirements for availability that may have been identified during the assessment. While there are no specific SLAs for serverless and resources for query execution are being acquired on-demand, currently there is 30 min timeout for query execution. Identify the longest expected running queries from your assessment and validate against your serverless SQL design. Be aware that a 30 min timeout may break the expectations for your workload and appear to be a service problem.\n  Consistency: SQL Serverless is primarily for read workloads, validate if all consistency checks have been performed during the Data Lake data provisioning and formation process. Keep on your radar new capabilities like recent addition of Delta Lake open-source storage layer which provides support for ACID (Atomicity, Consistency, Isolation and Durability) guarantees for transactions and it allows you to implement effective lambda/kappa architectures to support both streaming and batch use cases. Evaluate your design for opportunities to apply new capabilities but not at the expense of your project\u0026rsquo;s timeline or cost.\n  Backup: Review from your assessment your disaster recovery requirements. Validate against your serverless SQL design for recovery. SQL Serverless itself does not have its own storage layer and that would require handling snapshots and backup copies of your data. The data store accessed by serverless SQL is external (ADLS storage) review the recovery design in your project for these datasets.\n  Security #  Organization of your data is also important for building flexible security foundations: in most cases different processes and users will require different permissions and access to specific sub-areas of your Data Lake or logical Data Warehouse.\n  Data Storage: Using the information gathered during the assessment identify if typical Raw, Stage and Curated data lake areas need to be placed on the same Storage Account vs independent Storage Accounts. The latter might result in more flexibility in terms of roles and permissions and also add additional IOPS capacity that might be needed if your architecture must support heavy and simultaneous read/write workloads (potentially that could be a case of real time or IoT scenarios). Validate if you need to further segregate and keep your Sandboxed areas and Master Data areas on separate Accounts as well. Most users will not need to update and delete data and therefore do not need write permissions to your Data Lake, except Sandboxed and private areas. Evaluate what you have identified from the assessment against the design.\n  From your assessment information identify if any requirements point to features like Always Encrypted, Dynamic Data Masking or Row Level Security. Validate the availability of these features in specific scenarios - like with OPENROWSET \u0026ndash; Review against your design and anticipate potential workarounds that may be required.\n  From your assessment information identify what would be the best authentication methods, i.e. AAD Service Principals, SAS, when and how Authentication pass-through can be leveraged and integrated in the exploration tool of choice of the customer. Evaluate the design and validate that the best authentication method is part of the design.\n  Other considerations #  Review your design and check if you have put in place our best practices and recommendations (Best practices for serverless SQL pool - Azure Synapse Analytics ). Give special attention to filter optimization and proper collation to assure that predicate push-down works properly.\nConclusion #  Taking the time to evaluate your serverless SQL design against the information you gathered from the assessment will help ensure your SQ Serverless in Azure Synapse architecture is sound and meets your needs.\n"},{"id":19,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/solution-evaluation/is_sparkpooldesignevaluation/","title":"Is Spark Pool Design Evaluation","section":"Implementation Success","content":"Spark Pool Design Evaluation #  Overview #  Apache Spark in Synapse brings the Apache Spark parallel data processing to the Azure Synapse. This evaluation provides direction on when Apache Spark in Azure Synapse is or is not the best fit for your workload and will discusses items to consider when you are evaluating your solution design elements that incorporate Spark Pools.\nFit Gap Analysis #  When planning to implement Spark pools with Azure Synapse, you first need to ensure Apache Spark Pools in Azure Synapse is the best fit for your workload. These are the items to consider for utilizing Apache Spark in Synapse:\n  Does your workload require data engineering/data preparation?\n  Apache Spark works best for workloads that require:\n  Data Cleaning\n  Transforming semi-structured data like XML into relational\n  Complex free-text transformation (fuzzy matching, NLP)\n  Prepping data for machine learning\n      Is your workload for data engineering/data preparation complex or simple transformations? Are you looking for a low code/no code approach?\n  For simple transformations such as dropping of columns, casting of columns, or joining two datasets. Consider using Data Flows within Synapse Pipelines.\n  Data Flows also provides a low code/no code approach to your data processing.\n    Does your workload require machine learning on Big Data?\n Apache Spark works well for large datasets that will be used for machine learning. If using small datasets, consider using Azure Machine Learning as the compute.    Do you plan to perform data exploration and need an ad-hoc query analysis on Big Data?\n Apache Spark in Azure Synapse provides Python/Scala/SQL/.NET based data exploration. However, if you need full T-SQL experience consider using Synapse SQL Serverless.    Do you have a current Spark/Hadoop workload and need a unified Big Data Platform?\n  Azure Synapse provides a full unified analytical platform for working with Big Data from Spark to SQL Serverless for ad-hoc queries to SQL on dedicated pools for reporting and serving of data.\n  Moving from a Spark/Hadoop workload from on-prem or another cloud environment may require refactoring and should be considered.\n  If you are looking for a lift and shift approach for your Apache Big Data environment from on-prem to the cloud and require strict data engineering SLA, consider using HDInsight\n    Architecture Considerations #  To ensure that your Apache Spark pool meets your requirements for operational excellence, performance, reliability, and security there are key areas to validate in your architecture.\nOperational Excellence #    Environment: When configuring your environment ensure that you have designed your pool to take advantage of features such as auto-scale and dynamic allocation. Also consider utilizing the automatic pause feature to reduce costs.\n  Package Management: Determine if the Apache Spark libraries to be used will be used at a Workspace, Pool, or Session level. When managing packages at each level there are different considerations to consider.\n  Monitoring: Apache Spark in Azure Synapse provides built-in Spark pool and application monitoring with the creation of each spark session. Also consider implementing application monitoring with Azure Log Analytics or Prometheus and Grafana to visualize metrics and logs.\n  Performance Efficiency #    File size and File type: The size of the file and the type of file for reading and writing to and from Apache Spark to cloud storage is key to good performance. Both should be handled prior to ingestion to Apache Spark. Design the architecture to ensure that the file types are conducive to native ingestion with Apache Spark. Also ensure that the size of the files are large files and not many small files.\n  Partitioning: Identify if partitioning at the folder and/or file level will be implemented for your workload. Folder partitions limit the amount of data to be searched and read. File partitioning reduces the amount of data to be searched in the file but only applies to specific file formats and needs to be considered in the initial architecture.\n  Reliability #    Availability: Spark pools have a start time of three to four minutes. Depending on the number of libraries to be installed this could take longer. When designing batch vs. streaming workloads identify the SLA for executing the job from your assessment information and determine which architecture meets your needs.\n  In addition, each execution of a job creates a new Spark pool cluster. This should also be taken into consideration.\n  Checkpointing: Apache Spark has built-in checkpointing capabilities when using streaming workloads. This allows for your stream to recover from the last processed entry when there is a failure on a node in your pool.\n  Security #    Data: Data to be accessed must be considered for the ADLS account that is attached to the Synapse workspace. In addition, determine the security levels that should be required to access any data that is not within the Synapse environment. Refer to the information you collected during the assessment.\n  Networking: Review the networking information and requirements gathered during your assessment. If the design specifies to use a managed virtual network with Azure Synapse, consider the implications this will have on Apache Spark in Azure Synapse. One such consideration is the inability to use Spark SQL when accessing data.\n  Conclusion #  Utilizing the guidance discussed in this design evaluation and validating your design against these guidelines and the information you gathered during the assessment you will help ensure your Apache Spark in Azure Synapse architecture is sound and meets your needs. You can find more guidance on best practices when using Apache Spark in Azure Synapse here.\n"},{"id":20,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/solution-evaluation/is_workspacedesignevaluation/","title":"Is Workspace Design Evaluation","section":"Implementation Success","content":"Workspace Design Evaluation #  Overview #  Synapse Workspace is a unified graphical user experience that stitches together your analytical and data processing engines, data lakes, databases, tables, datasets, and reporting artifacts along with code and process orchestration. Considering the number of technologies and services that are integrated into Synapse Workspace make sure that the key components are included in your design.\nSynapse Workspace(s) Design Review #  Identify if your solution design is built using one or more than one Synapse Workspace. Identify what are the drivers of this design. While there might be a variety of different reasons, in most of the cases, the driver for multiple workspaces is either strict security guidelines or billing segregation. When determining the appropriate number of workspaces and database boundaries that are required be aware that there is a limit of 200 workspaces per subscription.\nIdentify what elements or services within each workspace need to be shared and with whom, like data lakes, integration runtime(s), metadata or configurations and code. Evaluate why this particular design has been chosen, what are potential synergies vs added costs and management overhead that the design may bring.\n[NOTE: If a development environment has been designed \u0026ndash; it will be evaluated in a separate step of this method as the design of the development environment is of critical importance to the success of your project.]\nData Lake Design Review #  It is recommended that the Data Lake (if part of the solution) be properly tiered: you want to divide your data lake into three major areas which contain Bronze, Silver and Gold datasets. Bronze or Raw layer might reside on its own separate Storage Account having strict/different security controls associated with it due to unmasked sensitive data that it might hold.\nSecurity Design Review #  Review the security design for the workspace and compare it with the information you gathered during the assessment to make sure all of the requirements are met, and all of the constraints have been taken into account. For ease of management is it is recommended that users be organized into groups with appropriate permissions profiling: access control can be simplified by using security groups that are aligned with people's job roles. You only need to add and remove users from appropriate security groups to manage access.\nServerless SQL pools and Apache Spark tables store their data in an ADLS Gen2 container associated with the workspace. User-installed Apache Spark libraries are also managed in this same storage account. To enable these use cases, both users and the workspace MSI must be granted Storage Blob Data Contributor access to this workspace ADLS Gen2 storage container \u0026ndash; verify this requirement against your security requirements.\nDedicated SQL Pools provide a rich set of security features to encrypt and mask sensitive data. Both dedicated and serverless SQL pools enable the full surface area of SQL Server permissions including built-in roles, user-defined roles, SQL authentication, and Azure Active Directory Authentication. Review the security design for the solution\u0026rsquo;s dedicated SQL pool and Serverless SQL pool access and data.\nReview the security plan for your Data Lake and all the ADLS storage (and other) that will be part of the Azure Synapse Analytics implementation for your solution. ADLS Data Lake storage is not itself a compute engine and thus has no built-in ability to selectively mask data attributes. ADLS Data Lake permissions can be applied at the storage account or container level using RBAC and/or at the folder or file level using ACLs. Review the design carefully and avoid unnecessary complexity.\nBe sure to consider the following during the security review:\n  Make sure configuring Azure Active Directory (AAD) is in the design.\n  Check for issues that may arise because some data is in another Azure tenant, needs to move to another tenant, or needs to be accessed by users in another tenant, etc. Ensure these scenarios are delt with in the design \u0026ndash; you should have gathered some insight into this during the assessment.\n  Who are the personas for each workspace \u0026ndash; how will they use the workspace?\n  How is the security designed within the workspace?\n  Who can view all scripts, notebooks, and pipelines?\n  Who can execute scripts and pipelines?\n  Who can create/pause/resume SQL and Spark pools?\n  Who can publish changes to the workspace?\n  Who can commit changes to source control?\n    Will pipelines access data using stored credentials or the workspace managed identity?\n  Do users have the appropriate access to the data lake to browse the data in Synapse Studio?\n  Is the Data Lake properly secured using the appropriate combination of RBAC and ACLs?\n  Have the SQL pool user permissions been determined for each type of user (data scientist, developer, administrator, business user, etc.)?\n  Networking Design Review #  Check the information from the assessment against the design:\n  Is connectivity designed between all the resources?\n  What is the networking mechanism to be used (ExpressRoute, Internet) private endpoints?\n  Do you need to be able to securely connect to Synapse Studio?\n  Has data exfiltration been taken into consideration?\n  Do you need to connect to on-prem data sources?\n  Do you need to connect to other cloud data sources or compute engines such as Azure ML?\n  Have Azure networking components like NGSs been reviewed for proper connectivity and data movement?\n  Has integrating with the private DNS zones been taken into consideration?\n  Do you need to be able to browse the data lake from within Synapse Studio or simply query data in the lake with serverless or PolyBase?\n  And finally, identify all your data consumers and verify that their connectivity is accounted for in the design. Check that network and security outposts allow your service to access required on-prem sources and its authentication protocols and mechanisms are supported. In some scenarios you might need to have more than one Self-Hosted Integration Runtime or Data Gateway for SaaS solutions like Power BI.\nMonitoring Design Review #  Review the design of the monitoring of the Azure Synapse components of your solution meet the requirements and expectations identified during the assessment. Verify that monitoring of resources and data access has been designed identifying each monitoring requirement (from assessment info) and matching it to the design: a proper monitoring solution should be put in-place as part of the first deployment to production to ensure failures are identified, diagnosed, and addressed in a timely manner. Aside from the base infrastructure and pipeline runs, data should also be monitored. Dependent upon the Azure Synapse components in use identify the monitoring requirements for each component \u0026ndash; For example: if Spark Pools are part of the solution, a common area that should have data monitoring is the malformed record store. Here are some items to consider in the monitoring design:\n  Who can monitor each resource type (pipelines, pools, etc.)?\n  How long do database activity logs need to be retained?\n  Will workspace and database log retention use Log Analytics or Azure Storage?\n  Will alerts be triggered in the event of a pipeline error? If so, who should be notified?\n  What thresholds for a SQL Pool should trigger an alert? Who should be alterted?\n  Source Control Design Review #  The default behavior for Synapse Analytics Workspace is to apply changes directly to the Synapse service using the built in Publish functionality. While optional, source control integration provides many advantages including better collaboration, versioning, approvals, and release pipelines to promote changes through dev/test/prod environments. Synapse allows a single source control repository per workspace which can be either Azure DevOps Git or GitHub.\nEnsure the following items have been considered in the source control design:\n  If using Azure DevOps Git, are the Synapse Workspace and repository in the same tenant?\n  Who will be able to access source control?\n  What permissions will each user be granted in source control?\n  Has a branching and merging strategy been developed?\n  Will release pipelines be developed for deployment to different environments?\n  Will an approval process be leveraged for merging and for release pipelines?\n  [NOTE: Dev/Test/Prod environment design will be evaluated in the Solution Development Environment Design Review as a separate step of this method. The design of the development environment is of critical importance to the success of your project.]\nConclusion #  This review has covered the Synapse Workspace aspects of the design review. Validating the overall, high-level networking, security, monitoring, and workspace design against the solution and environment requirements collected during the Assessment earlier in the Implementation Success methodology. Additional design specifics for Synapse components (SQL Pools, Spark Pools, Pipeline, etc.) will be addressed in more detail in following Implementation Success design reviews.\n"},{"id":21,"href":"/Synapse-Success-By-Design/docs/implementation-success/project-planning/team-skillset-evaluation/is_evaluateprojectteamreadiness/","title":"Is Evaluate Project Team Readiness","section":"Implementation Success","content":"Evaluate Project Team Readiness #  Overview #  Solution development will require a variety of resources possessing a variety of skills. It will be very important for the success of your solution that these resources have the necessary skills to successfully complete the tasks assigned. The following evaluation will take an honest and critical look at the skill level of your project resources and provide you with a list of roles that are commonly needed during the implementation of a solution incorporating Azure Synapse. Resources will need to possess the appropriate experience and level of skill in the appropriate technology to complete their assigned project tasks within the time frame expected and meet the quality bar required for your project\u0026rsquo;s success.\nMicrosoft Standard Level Definitions #  For this evaluation we will use the standard Microsoft Level of understanding definitions for evaluation of individual skill levels.\nLevel 100 (L100) #  Introductory and overview material. Assumes little or no expertise with topic and covers topic concepts, functions, features, and benefits.\nLevel 200 (L200) #  Intermediate material. Assumes 100-level knowledge and provides specific details about the topic.\nLevel 300 (L300) #  Advanced material. Assumes 200-level knowledge, in-depth understanding of features in a real-world environment, and strong coding skills. Provides a detailed technical overview of a subset of product/technology features, covering architecture, performance, migration, deployment, and development.\nLevel 400 (L400) #  Expert material. Assumes a deep level of technical knowledge and experience and a detailed, thorough understanding of the topic. Provides expert-to-expert interaction and coverage of specialized topics.\nRoles, Resources and Readiness #  There are multiple roles and skillsets needed for a successful Azure Synapse implementation.\nHere is a set of roles commonly required to implement a successful project that incorporates Azure Synapse. Not all of the roles identified will be required for all projects and not all of the roles identified will be required for the full duration of the project, however, they will be needed to complete some critical project tasks and the skill level of the individual(s) executing on the task needs to be evaluated to assure their success in completing their task. Please refer to your project plan and verify that these resources/roles have been identified in the effort and cost and see if your project plan identifies additional resources/roles. In many cases you may find that a single individual works in more than one of the roles listed below (Example: your Azure Administrator is also your Azure Network Administrator) or that a role in your organization is split between multiple people (Example: Your Synapse Administrator does not do the Synapse SQL Security) so adjust your evaluation accordingly.\nUsing the roles below, you project plan and you assessment information:\n  Identify the roles that will be required by your solution implementation\n  Identify the specific individuals in your project that will fulfill each role\n  Identify the specific project tasks that will be performed by the individual\n  Assign each individual a Level of Understanding as described in the above section for the assigned tasks and role(s)\n  For the best chance of a successful implementation, each individual working in each role should be at least a L300 for the project tasks to be performed. It is strongly recommended that individuals at L200 or below be provided the necessary guidance and instruction to raise their level of understanding prior to beginning their project tasks or that a L300 or higher individual is identified to assist, as necessary. It is recommended that a reasonable amount of time to learn any new skill and ramp-up on a skill be considered in the project plan timeline and effort estimates.\nAzure Administrator #  Manages administrative aspects of Azure - Subscriptions, Region Identification, Resource Groups, Azure Monitoring, Portal Access, provisioning of resources (Resource Groups, Storage Accounts, Azure Data Factory, Azure Purview, Etc.)\nSecurity Administrator #  Collaborates with Synapse Workspace admin, Synapse Database Admin, Synapse Spark Admin, and others to configure security to align with security requirements.\nAzure Active Directory (AAD) Admin\nLocal knowledge of the existing security landscape and requirements\nNetwork Administrator #  Azure Networking skills\nSynapse Networking Skills\nLocal knowledge of the existing enterprise networking environment and requirements\nSynapse Workspace Administrator/Synapse Administrator #  A Synapse Administrator is responsible for the administration of the overall Azure Synapse environment. They are responsible for the availability and scale of Workspace resources, Data Lake administration, Analytics Pools, Workspace administration and monitoring.\nThe Azure Synapse Administrator will work closely with all roles to ensure Synapse access, Analytics services availability, and scale.\nSome activities include:\n  Provisions Synapse Workspaces\n  Configures Synapse Networking and Security configuration\n  Monitors activity Synapse Workspaces\n  Synapse Database Administrator #  A Synapse database administrator is responsible for the design, implementation, maintenance, and operational aspects of the Synapse SQL Pools (Serverless and Dedicated). This administrator is responsible for the overall availability and consistent performance and optimizations of the Synapse Pools.\nThe database administrator is also responsible for managing the security of the data in the databases, granting privileges over the data, granting, or denying access to users as appropriate.\nSome activities include:\n  Performs admin functions (Provisions, scales, pauses, resumes, restores, workload management, monitoring, etc.) for dedicated SQL Pools\n  Performs admin functions (securing, monitoring, etc.) for serverless SQL Pools\n  Configures SQL Pool database security\n  Performance Tuning and Troubleshooting\n  Synapse Spark Administrator #  A Synapse Spark administrator is responsible for the design, implementation, maintenance, and operational aspects of the Synapse Spark Pools. This administrator is responsible for the overall availability and consistent performance and optimizations for the Synapse Spark Pools.\nThe Spark administrator is also responsible for managing the security of the data, granting privileges over the data, granting, or denying access to users as appropriate.\nSome activities include:\n  Performs Spark Pool admin functions (provisions, monitors, etc.) for Spark pools\n  Security of data and pools\n  Notebook troubleshooting and performance\n  Pipeline Spark execution troubleshooting and performance\n  Synapse SQL Pool Database Developer #  Database design and development (table structure and indexing, developing database objects, Schema design, logical and physical database design) for dedicated SQL Pools.\nDatabase design and development (External table access, views, schema design) for serverless SQL Pools\nSome activities include:\n  Logical and Physical database design\n  Table design including distribution, indexing, and partitioning\n  Programming object design and development (stored procedures, functions)\n  Design and development of additional performance optimizations (Materialized Views, Workload Management, Etc.)\n  Design and implementation of security features (Dynamic Data Masking, Row Level Security, Column Level Security, Encryption, Etc.)\n  Monitoring, Auditing, Performance Tuning and Troubleshooting\n  Spark Developer #  Some activities include:\n Creates Notebooks and executes Spark processing using Synapse Spark Pools  Data Integration Admin #  (Synapse Pipelines, ADF (AZURE DATA FACTORY), 3rd party integration tools)\nPerforms all the configuration and security functions to support your data integration tool(s)\nFor ADF or Azure Pipelines this will include configuration of Integration Runtime (IR), Shared Integration Runtime (SHIR), and/or SSIS Integration Runtime (SSIS-IR). Knowledge of VM provisioning and configuration on-prem and in Azure may be required, review your design and project plan.\nData Integration Developer #  Develop ETL/ELT and other data integration processes using the solution\u0026rsquo;s selected data integration tool(s)\nData consumption tool(s) Admin #  PowerBI, Tableau, Excel, Custom applications, etc. - all the data consumption tools that are part of your solution and accessing your data. You will need an Admin for each tool to complete some of your project tasks. Including configuration and security for the tool to access data in Synapse and Azure.\nData Engineer #  A data engineer designs and implements data-related assets that include data ingestion pipelines, cleansing and transformation activities, and data stores for analytical workloads. They use a wide range of data platform technologies, including relational and nonrelational databases, file stores, and data streams.\nThey are also responsible for ensuring that the privacy of data is maintained within the cloud and spanning from on-premises to the cloud data stores. They also own the management and monitoring of data stores and data pipelines to ensure that data loads perform as expected.\nData Scientist #  Data scientists are necessary to derive value and insight from data. Data scientists find innovative ways to work with data and help teams achieve a rapid ROI on analytics efforts using methods including data curation or advanced search, matching, and recommendation algorithms. Data scientists need access to the highest quality of data and substantial amounts of computing resources to extract deeper insights.\nData Analyst #  Data analyst enables businesses to maximize the value of their data assets. They are responsible for designing and building scalable models, cleaning, and transforming data, and enabling advanced analytics capabilities through reports and visualizations.\nA data analyst processes raw data into relevant insights based on identified business requirements to deliver relevant insights.\nAzure DevOps Engineer #  An Azure DevOps Engineer is responsible for designing and implementing strategies for collaboration, code, infrastructure, source control, security, compliance, continuous integration, testing, delivery, monitoring of an Azure Synapse project.\nIn addition to the above list of roles you may wish to also review the built-in RBAC roles for Azure Synapse and the RBAC roles built into Azure to align your list of roles to those within Azure and Azure Synapse. Be aware that these are two independent sets of RBAC roles and permissions.\n#  Learning Resources and Certifications #  If you are interested in Microsoft Certifications that may help assess your team\u0026rsquo;s readiness browse the available certifications for Azure and Synapse at Microsoft Learn.\nFor some materials to improve your teams readiness browse the available Azure Synapse Analytics Learning Paths here on Microsoft Learn.\nConclusion #  All team members play a key role during a Synapse Analytics project weather they are full time for the duration of the project or only needed to perform one or two tasks. Identifying the project resources and evaluating their readiness is an important task for a successful Synapse Analytics Project. If you do not have skilled members, secure additional members, or skill-up team members.\n"},{"id":22,"href":"/Synapse-Success-By-Design/docs/implementation-success/solution-development/is_solutioncheckup/","title":"Is Solution Check Up","section":"Implementation Success","content":"#  Coming Soon Solution Checkup #  "}]