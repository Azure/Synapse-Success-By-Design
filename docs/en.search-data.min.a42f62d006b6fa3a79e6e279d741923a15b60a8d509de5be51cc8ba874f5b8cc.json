[{"id":0,"href":"/Synapse-Success-By-Design/docs/home/","title":"Home","section":"Docs","content":"Welcome to the Azure Synapse Analytics Content Repository. In this repository we have compiled the content that we have found helpful on our engagements with customers.\nThis repository is intended to be a supplement to the official Azure Synapse documentation with additional specialized content and links to non-Microsoft generated content. We encourage you to review the official documentation also.\nSuccess By Design #  The Success By Design Implementation method is designed to help guide you to a successful solution implementation that includes Azure Synapse Analytics as a component. This method is designed to complement your solution implementation project by adding suggested checks at strategic points during your project that can help assure a successful implementation of Azure Synapse Analytics. This method does not replace or change your chosen project management method (SCRUM, Agile, Waterfall), but will suggest validations to make while your project is in flight to improve the success of your overall deployment to a production environment.\nAzure Synapse is an enterprise analytics service that accelerates time to insight across data warehouses and big data systems. Azure Synapse brings together the best of SQL technologies used in enterprise data warehousing, Spark technologies used for big data, Pipelines for data integration and ETL/ELT, and deep integration with other Azure services such as Power BI, CosmosDB, and AzureML.\nWhat is Azure Synapse Analytics? - Azure Synapse Analytics | Microsoft Docs The Synapse Implementation Success method uses a strategic checkpoint approach to assessing and monitoring a project\u0026rsquo;s progress.\nThe goals of these checkpoints include:\n  Proactive identification of possible issues and blockers\n  Continuous validation of the solution\u0026rsquo;s fit to the use case\n  Successful deployment to production\n  Smooth operation and monitoring once in production\n  Synapse Implementation Success checkpoints are invoked at four key places within the Implementation Project:\n  Project Planning\n  Solution Development\n  Pre Go-Live\n  Post Go-Live\n  Sections #   Synapse Implementation Success Strategic Checkpoints  Project Planning Solution Development Pre Go-Live Post Go-Live   Conclusion   Synapse Implementation Success Strategic Checkpoints #  Brief outlines of the checkpoints that occur within these four strategic project phases. One paragraph - why here? how will it help? Short list of supporting checklists, documents, whatever we are going to be creating for use.\nProject Planning #  Solution Evaluation #  The Solution in total will need to be evaluated with a focus on how it is making use of Azure Synapse as part of the architecture. This assessment will gather data that will identify the components of Azure Synapse that are being used, the interfaces each will have with other products, review the data sources, the data consumers, the personas, and use cases. The assessment will identify which Synapse components will be implemented and thus what evaluations and check points should be made throughout the implementation effort.\n  Perform an Assessment of the solution\n  Identify the Azure Synapse Analytics solution components\n  Execute Evaluations based upon the solution components\n  Review the results of these evaluations and respond accordingly\n  Project plan Evaluation #  Evaluate the project plan as it relates to the Azure Synapse Analytics work that needs to be completed. This evaluation is not to make project plan changes but to identify steps that may be missing that could lead to blockers and impact the planned timeline. Once evaluated, the project plan may need to be modified and updated in response to the findings.\n  Evaluate Project Plan at a high level.\n  Evaluate project planning specific to the solution architecture components of Azure Synapse Analytics being implemented.\n  Review the results and adjust accordingly.\n  Assess Team Readiness and Training #  Evaluate the project team in the aspect of their skill level and readiness to implement the solution incorporating Azure Synapse Analytics.\nIt will be important for the project’s success to have the correct skillsets. There are many and differing skillsets required for a successful implementation using Azure Synapse Analytics. Take time at this point to identify gaps and secure resources with the required skillset or take the time to complete training.\nThis evaluation is critical at this stage. The lack of the proper skillset will impact both the timeline and the overall success of the solution.\n  Evaluate the team Azure Synapse Analytics Skillset\n  Secure additional skilled team members or skill-up team members\n  Back to Sections\nSolution Development #  Periodic Quality Checks #  Throughout the solution development checks should be made periodically to validate that the solution being built with respect to recommended practices as they apply to the different components of Azure Synapse Analytics. Checking that the project use cases will be satisfied, and the enterprise requirements are being met. For the purposes of this method, we are calling these periodic evaluations quality checks.\nImplement the following quality checks as appropriate to your solution.\n  Quality Checks for Workspaces\n  Quality Checks for Data Integration\n  Quality Checks for dedicated SQL Pools\n  Quality Checks for serverless SQL Pools\n  Quality Checks for Spark Pools\n  Back to Sections\nPre Go-Live #  Prior to deploying your solution to production, the following reviews are recommended to assess the preparedness of the solution and the organization for adoption and support of the solution on Azure Synapse Analytics. The Go-Live checklists for the specific Azure Synapse Analytics components will provide a final check of readiness to successfully move to production.\n  Operational Readiness Review\n  Continued Dev (CI/CD etc.) Preparedness\n  User Readiness and onboarding plan Review\n  Complete the Go-Live checklist for Workspaces\n  Complete the Go-Live checklist for Data Integration\n  Complete the Go-Live check list(s) fitting your Azure Synapse Analytics solution\n  Dedicated SQL Pool\n  Serverless SQL Pool\n  Spark Pool\n    Back to Sections\nPost Go-Live #  Content coming soon\nBack to Sections\n Conclusion #  The Success By Design Synapse Implementation Success method is provided to help you with your success in implementing a solution incorporating Azure Synapse Analytics. By utilizing this method during your Azure Synapse Analytics implementation project, you should be able to identify and address any issues early on and deploy to production a solution that benefits your business and delights your users.\nBack to Sections\n"},{"id":1,"href":"/Synapse-Success-By-Design/docs/poc-playbooks/","title":"Poc Playbooks","section":"Docs","content":"##playbooks\n"},{"id":2,"href":"/Synapse-Success-By-Design/docs/success-by-design/","title":"Success by Design","section":"Docs","content":"Success By Design : Synapse Implementation Success #  Overview #  The Success By Design: Synapse Implementation Success method is designed to help guide you to a successful solution implementation that includes Azure Synapse Analytics as a component. This method is designed to complement your solution implementation project by adding suggested checks at strategic points during your project that can help assure a successful implementation of Azure Synapse Analytics. This method does not replace or change your chosen project management method (SCRUM, Agile, Waterfall), but will suggest validations to make while your project is in flight to improve the success of your overall deployment to a production environment.\nAzure Synapse is an enterprise analytics service that accelerates time to insight across data warehouses and big data systems. Azure Synapse brings together the best of SQL technologies used in enterprise data warehousing, Spark technologies used for big data, Pipelines for data integration and ETL/ELT, and deep integration with other Azure services such as Power BI, CosmosDB, and AzureML.\nThe Synapse Implementation Success method uses a strategic checkpoint approach to assessing and monitoring a project\u0026rsquo;s progress.\nThe goals of these checkpoints include:\n Proactive identification of possible issues and blockers Continuous validation of the solution\u0026rsquo;s fit to the use case Successful deployment to production Smooth operation and monitoring once in production  Synapse Implementation Success checkpoints are invoked at four key places within the Implementation Project:\n Project Planning Solution Development Pre Go-Live Post Go-Live  Synapse Implementation Success Strategic Checkpoints #  Here are the strategic checkpoints we recommend during these four project phases\nProject Planning #  Solution Evaluation #  The Solution in total will need to be evaluated with a focus on how it is making use of Azure Synapse as part of the architecture. This assessment will gather data that will identify the components of Azure Synapse that are being used, the interfaces each will have with other products, review the data sources, the data consumers, the personas, and use cases. The assessment will identify which Synapse components will be implemented and thus what evaluations and check points should be made throughout the implementation effort.\n Perform an Assessment of the solution Identify the Azure Synapse Analytics solution components Execute Evaluations based upon the solution components Review the results of these evaluations and respond accordingly  Project plan Evaluation #  Evaluate the project plan as it relates to the Azure Synapse Analytics work that needs to be completed. This evaluation is not to make project plan changes but to identify steps that may be missing that could lead to blockers and impact the planned timeline. Once evaluated, the project plan may need to be modified and updated in response to the findings.\n Evaluate Project Plan at a high level. Evaluate project planning specific to the solution architecture components of Azure Synapse Analytics being implemented. Review the results and adjust accordingly.  Assess Team Readiness and Training #  Evaluate the project team in the aspect of their skill level and readiness to implement the solution incorporating Azure Synapse Analytics.\nIt will be important for the project’s success to have the correct skillsets. There are many and differing skillsets required for a successful implementation using Azure Synapse Analytics. Take time at this point to identify gaps and secure resources with the required skillset or take the time to complete training. This evaluation is critical at this stage. The lack of the proper skillset will impact both the timeline and the overall success of the solution.\n Evaluate the team Azure Synapse Analytics Skillset Secure additional skilled team members or skill-up team members  During Solution Development #  Periodic Quality Checks #  Throughout the solution development checks should be made periodically to validate that the solution being built with respect to recommended practices as they apply to the different components of Azure Synapse Analytics. Checking that the project use cases will be satisfied, and the enterprise requirements are being met. For the purposes of this method, we are calling these periodic evaluations quality checks. Implement the following quality checks as appropriate to your solution.\n Quality Checks for Workspaces Quality Checks for Data Integration Quality Checks for dedicated SQL Pools Quality Checks for serverless SQL Pools Quality Checks for Spark Pools  Recommend Additional Skill Building #  As the project progresses identify if additional skills are needed. Take the time to get the additional skillset to improve the quality of the solution and avoid project delays and project timeline impacts.\nPre Go-Live #  Prior to deploying your solution to production, the following reviews are recommended to assess the preparedness of the solution and the organization for adoption and support of the solution on Azure Synapse Analytics. The Go-Live checklists for the specific Azure Synapse Analytics components will provide a final check of readiness to successfully move to production.\n Operational Readiness Review Continued Dev (CI/CD etc.) Preparedness User Readiness and onboarding plan Review Complete the Go-Live checklist for Workspaces Complete the Go-Live checklist for Data Integration Complete the Go-Live check list(s) fitting your Azure Synapse Analytics solution  Dedicated SQL Pool Serverless SQL Pool Spark Pool    Post Go-Live #  Following the deployment to production the following reviews are recommended to assure that the solution is operating as expected now that it is in production. Select the reviews appropriate to the solution\n Monitoring Review Analytics Health Review  Dedicated SQL Pool Serverless SQL Pool Spark Pools    Conclusion #  The Success By Design Synapse Implementation Success method is provided to help you with your success in implementing a solution incorporating Azure Synapse Analytics. By utilizing this method during your Azure Synapse Analytics implementation project, you should be able to identify and address any issues early on and deploy to production a solution that benefits your business and delights your users.\n"},{"id":3,"href":"/Synapse-Success-By-Design/docs/whats-new/","title":"Whats New","section":"Docs","content":"Whats new! #  "},{"id":4,"href":"/Synapse-Success-By-Design/docs/pools/dedicated/","title":"Dedicated Pools","section":"Docs","content":"SQL Provisioned Pool #  Azure Synapse Analytics is an analytics service that brings together enterprise data warehousing and Big Data analytics. Dedicated SQL pool (formerly SQL DW) refers to the enterprise data warehousing features that are available in Azure Synapse Analytics.\nSynapse SQL leverages a scale out architecture to distribute computational processing of data across multiple nodes. Compute is separate from storage, which enables you to scale compute independently of the data in your system.\nFor dedicated SQL pool, the unit of scale is an abstraction of compute power that is known as a data warehouse unit.\nReview: Dedicated SQL pool architecture - Azure Synapse Analytics | Microsoft Docs\nTo determine if Synapse is a good fit for a migration, we need to understand what is currently being used in the original system and determine if there are Synapse gaps that can be a potential blocker.\nSynapse Assessment #  Base Checklist:\n Identity the Source Workload Type (DW/OLTP)?    Identity the Source ETL/ELT Tool\n  Does it support Synapse?\n  How much work is needed to move from Source to Synapse?\n      Identify the Data Sources\n  Identify the Data Ingestion processes\n  Ad-Hoc?\n  Batch\n  Files\n  Stream\n  Etc.\n      Data Consumption Platform\n  Does the current Tool support Synapse?\n  Type of access (DQ/Import)\n  Max users / Concurrent users\n      Synapse Sizing (How do choose Synapse DWU)\n  Identify Source Number of Nodes/Memory/CPUs\n  Source Data Volume\n  Source Concurrency\n  Determine “Active Data Set”\n      Feature Gaps\n  Identify unsupported data types (ex. Xml, arrays, spatial).\n  Identify unsupported features (ex: Cursors)\n  Are there gaps without a work around?\n    @Tammy\nWe have an internal chart with the Synapse sizing recommendations, but I don’t know if it’s public or not.\nDefine Target Architecture #  Add url for MDW Architectures.\nReview: Azure Data Architecture Guide - Azure Architecture Center | Microsoft Docs\nReuse and Re-Write?\nThe purpose of the following is to try to reduce blockers and avoid some issues related to Synapse SQL Serverless pools frequently found on the course of the implementation of modern data architectures based on Azure Synapse or later in the production stages. To be consistent with guiding tenets of Well Architected Framework (Microsoft Azure Well-Architected Framework - Azure Architecture Center | Microsoft Docs) that aim to improve the quality of a workload we are going to try to reuse the main pillars that will allow better structure of the information and also help to introduce necessary checkpoints in the corresponding phases of you project execution.\nMigration #  Review: Azure Synapse Analytics: Migration guide - Azure Synapse Analytics | Microsoft Docs\nProject Team #  Topic: The project must have the right number of people with the right skillset (Synapse is not SQL)\nBest Practices and recommendations #  Please review our best practices from official documentation\n  Best practices for dedicated SQL pools - Azure Synapse Analytics | Microsoft Docs\n  Instead of ETL, design ELT - Azure Synapse Analytics | Microsoft Docs\n  Data loading best practices - Azure Synapse Analytics | Microsoft Docs\n  Development best practices for Synapse SQL - Azure Synapse Analytics | Microsoft Docs\n  Performance tuning with result set caching - Azure Synapse Analytics | Microsoft Docs\n  Designing tables - Azure Synapse Analytics | Microsoft Docs\n  Using T-SQL loops - Azure Synapse Analytics | Microsoft Docs\n  Using stored procedures - Azure Synapse Analytics | Microsoft Docs\n  Optimizing transactions - Azure Synapse Analytics | Microsoft Docs\n  Workload management - Azure Synapse Analytics | Microsoft Docs\n  Manageability and monitoring - overview - Azure Synapse Analytics | Microsoft Docs\n  Build integrated solutions - Azure Synapse Analytics | Microsoft Docs\n  Scope and work sprints #  Make sure that you define the scope of the work, aligned with a project plan with deliverables and milestones, to avoid unwanted surprises during the delivery.\nEngage a project manager into the team to assure that all work is correctly tracked, and that you are not getting into delays.\nTesting methodology #  Success Criteria #  "},{"id":5,"href":"/Synapse-Success-By-Design/docs/pools/serverless/","title":"Serverless Pools","section":"Docs","content":"SQL Serverless Pool #  Separation of storage and compute as a design principle for modern data and analytical platforms and services has been a trend and frequently used pattern since it provided cost savings and more flexibility in on-demand scaling of your storage and compute independently. Synapse SQL Serverless (Serverless SQL pool - Azure Synapse Analytics | Microsoft Docs) further extends this pattern and adds an important capability to query your data directly from your data lake storage and not to worry about compute management while leveraging self-service types of workload.\nThe purpose of the following is to provide core guidance for implementations and architectures that include Synapse SQL Serverless pools and try to reduce blockers and frequently found on the course of the implementation of modern data architectures based on Azure Synapse or later in the production stages. To be consistent with guiding tenets of Well Architected Framework (Microsoft Azure Well-Architected Framework - Azure Architecture Center | Microsoft Docs) that aim to improve the quality of a workload we are going to try to reuse the main pillars that will allow better structure of the information and also help to introduce necessary checkpoints in the corresponding phases of you project execution.\nBefore we dig into each one of the categories - Operational Excellence, Performance Efficiency, Reliability and Security - make sure you have a clear understanding of a future workload that you are aiming to land on SQL Serverless. While in many cases pay-per-query and autoscaling capabilities of querying data directly from your data lake storage are frequent motivation points, design should fit the purpose therefore validation of the type/s of workload would be a good starting point and the first suggested checkpoint.\nIn the scope of the architecture, we typically expect data science, exploratory analysis and self service purposes to be the main drivers for SQL Serverless along with a need for flexible data sharing scenarios across frameworks and technologies, especially when there are some operational or technical impediments to extend ETL/ELT processes for loading/unloading data to and from different engines and data stores.\nSome use cases will drive extremely high concurrency to your Synapse SQL Dedicated pools service (streaming or IoT with considerable number of independent processing threads that produce write operations). That scenario could potentially be a good fit for leveraging Data Lake and SQL Serverless for part of your workload (raw area stream writes to Data Lake and SQL Serverless based access to storage that requires most recent data).\nYou want to validate if fast data ingestion, more immediate data availability for exploration or reporting, flexibility in tools or frameworks that need to deal with the data are required and relaxed SLAs fit your use case/s and requirements. In many cases you will find a good fit when Synapse SQL and Synapse Spark (What is Apache Spark - Azure Synapse Analytics | Microsoft Docs) are being used on the same data and there is no need to add additional complexity. Read more about sharing metadata between SQL Serverless and Synapse Spark engine (Shared metadata model - Azure Synapse Analytics | Microsoft Docs)\n1.- Operational Excellence\n Environment: check if a single Synapse workspace fits your security and organizational requirements. More than one might be necessary in case you are obliged to segregate your production and dev/test users and workloads.\nModel and change management: if more than a single Synapse environment is required, consider placing critical scripts and DDLs onto a repository and sync the environments via pull requests.\nDeployment: SQL Serverless per se does not require any special deployment actions and you will have that service available on-demand with every Synapse workspace. Check regional proximity of the service and that of the Data Lake Storage Gen2 you access with SQL Serverless.\nTesting: check if your testing routines can be put and automated as part of your deployment pipelines by leveraging services like Azure DevOps. If certain testing is bound to performance, check and ensure stats have been created first by executing the benchmark workload beforehand.\nMonitoring: check if build-in monitoring is sufficient and if some external services need to put in place to store historical log data, in order to be able to analyze changes in performance or allow you to define alerting or triggered actions for specific situations.\n 2.- Performance Efficiency\n Unlike traditional database engines, SQL Serverless does not rely on its own optimized storage layer and for that reason its performance is strongly dependent on how data is organized in Azure Data Lake Storage Gen2.\nHaving a design and organization of your data files that projects good partitioning of your data, you can expect reliable performance of SQL Serverless, however the dedicated SQL pools usually would outperform the former and provide more flexibility in terms of tuning capabilities and workload management. Consider this point carefully when you have strict performance SLAs.\nData Ingestion: check how data is stored in Data Lake Storage Gen2, i.e., the file sizes, number of files, and folder structure have an impact on performance. Keep in mind that while some file sizes might work for SQL Serverless, but on the other side might impose troubles for efficient processing or consumption by other engines and applications.\nData Placement: check if you can unify and define common pattern for data placement and make sure that directory branching can support your security requirements. There are a few common patterns that help you keep your time series data organized. Whichever your choice is, make sure that it also works with other engines and workloads like if it can help partition auto-discovery for Spark applications and external tables.\nData Formats: there are number of choices and most probably you will be dealing with different formats on different layers of your Data Lake as each layer might need to support distinct types of workloads (like heavy writes or where schema evolves frequently vs others that should be more oriented for efficient exploration). Today SQL Serverless in most cases will offer the best performance and better compatibility feature-wise with Parquet format. Check your performance and compatibility requirements what format works best in your case, because while Parquet improves performance through better compression and reduction of IO by reading only required columns needed for the analysis, it requires additional compute and some source systems do not natively support Parquet for export output format which would lead for additional transformation steps in your pipelines and/or dependencies in your overall architecture.\nData transformations and conversions into desirable format can be easily and very efficiently achieved with Synapse Spark Pools - check if you can leverage that service in your data pipelines.\nExploration: every industry is different, but in many cases, there are certain data access patterns that will be common across most frequent queries of your workload., i.e. filtering and aggregations by dates, categories or geographies that are core to customers business. Check your most common filtering criteria, relate these to how much data is being read/discarded by top used queries and validate if the information on Data Lake is organized that favors your exploration performance expectations.\nCheck in your queries if you effectively eliminate unnecessary partitions in your OPENROWSET path parameter or, in case of external tables, if additional indexing is required.\nBear in mind that some tools like Power BI in addition can compensate and provide an additional layer of caching (Query caching in Power BI Premium - Power BI | Microsoft Docs) and compression techniques to fit necessary datasets in memory and improve end user experience in case of SQL Serverless - evaluate if tools with such characteristics need to be also included in your architecture from the beginning.\n 3- Reliability\n Availability: check if there are strict requirements for availability. While there are no specific SLAs for serverless and resources for query execution are being acquired on-demand, currently there is 30 min timeout for query execution which might break the expectations for your workload and appear to be a service problem. In such cases check if some of the best practices have not been applied.\nConsistency: SQL Serverless is mostly for read workloads, validate if all consistency checks have been performed during the Data Lake data provisioning and formation process. Keep on your radar new capabilities like recent addition of Delta Lake open-source storage layer which provides support for ACID (Atomicity, Consistency, Isolation and Durability) guarantees for transactions and it allows us to implement effective lambda/kappa architectures to support both streaming and batch use cases.\nBackup: weigh on your disaster recovery requirements: SQL Serverless does not have its own storage layer and that would require handling snapshots and backup copies of your data mostly externally (except situations when you can use CETAS to make copies of your critical data referenced by external tables.). In many Data Lake implementations there is always a way to recover/rebuild your data from RAW (should be immutable) layers where you should keep the required amount of historical data that could cover your worst-case scenarios.\n 4- Security\n Organization of your data is also important for building flexible security foundations: in most cases different processes and users will require different permissions and access to specific sub-areas or your Data Lake or logical Data Warehouse.\nData Storage: Check if typical Raw, Stage and Curated data lake areas need to be placed on the same Storage Account vs independent Storage Accounts. The latter might result in more flexibility in terms of roles and permissions and also adds additional IOPS capacity that might be needed in case your architecture must support heavy and simultaneous read/write workloads (potentially that could be a case of real time or IoT scenarios). Validate if you need to further segregate and keep your Sandboxed areas and Master Data areas on separate Accounts as well. Most of the users will not need to update and delete data and therefore do not need write permissions to your Data Lake, except Sandboxed and private areas.\nCheck what requirements point down to features like Always Encrypted, Dynamic Data Masking or Row Level Security and availability of these in a specific scenario - like with OPENROWSET - and anticipate potential workarounds in case these requirements might be blocked by current feature availability.\nAdditionally, check what would be the best authentication methods, i.e. AAD Service Principals, SAS, when and how Authentication pass-through can be leveraged and integrated in the exploration tool of choice of the customer.\n 5- Other considerations\n Best Practices: check if you have put in place our best practices and recommendations (Best practices for serverless SQL pool - Azure Synapse Analytics | Microsoft Docs). Special attention for filter optimization and proper collation for predicate push-down to work properly.\nService limits: additionally, check for current service limits like number of SQL Serverless Pools (1) per workspace, max query execution time (30 min), max number of columns that you can handle with OPENROWSET (1024), or what T-SQL surface area is supported, DML statements, etc.\n "},{"id":6,"href":"/Synapse-Success-By-Design/docs/pools/spark-pool/","title":"Spark Pools","section":"Docs","content":"SBD – Spark Pool Design Evaluation #  Apache Spark in Synapse brings the Apache Spark parallel data processing to the Azure Synapse.\nFit Gap Analysis #  When planning to implement Spark pools with Azure Synapse, you first need to ensure Apache Spark Pools in Azure Synapse is the best fit for your workload. These are the items to consider for utilizing Apache Spark in Synapse:\n  Does your workload require data engineering/data preparation?\n  Apache Spark works best for workloads that require:\n  Data Cleaning\n  Transforming semi-structured data like XML into relational\n  Complex free-text transformation (fuzzy matching, NLP)\n  Prepping data for machine\n      Is your workload for data engineering/data preparation complex or simple transformations? Are you looking for a low code/no code approach?\n  For simple transformations such as dropping of columns, casting of columns, or joining two datasets. Consider using Data Flows within Synapse Pipelines.\n  Data Flows also provides a low code/no code approach to your data processing.\n    Does your workload require machine learning on Big Data?\n Apache Spark works well for large datasets that will be used for machine learning. If using small datasets, consider using Azure Machine Learning as the compute.    Do you plan to perform data exploration and need an ad-hoc query analysis on Big Data?\n Apache Spark in Azure Synapse provides Python/Scala/SQL/.NET based data exploration. However, if you need full T-SQL experience consider using Synapse SQL Serverless.    Do you have a current Spark/Hadoop workload and need a unified Big Data Platform?\n  Azure Synapse provides a full unified analytical platform for working with Big Data from Spark to SQL Serverless for ad-hoc queries to SQL on dedicated pools for reporting and serving of data.\n  Moving from a Spark/Hadoop workload from on-prem or another cloud environment may require refactoring and should be considered.\n  If you are looking for a lift and shift approach for your Apache Big Data environment from on-prem to the cloud and require strict data engineering SLA, use HDInsight\n    Architecture Considerations #  To ensure that your Apache Spark pool meets your operational, performance, and security requirements, there are key areas to address in your architecture.\nOperational Excellence #  Environment Configuration\nWhen configuring your environment ensure that you have designed your pool to take advantage of features such as auto-scale and dynamic allocation. Also consider utilizing the automatic pause feature to reduce costs.\nPackage Management\nDetermine if the Apache Spark libraries to be used will be used at a Workspace, Pool, or Session level. When managing packages at each level there are different considerations to consider.\nMonitoring\nApache Spark in Azure Synapse provides built-in Spark pool and application monitoring with the creation of each spark session. Also consider implementing application monitoring with Azure Log Analytics or Prometheus and Grafana to visualize metrics and logs.\nNetworking\nIf designing to use a managed virtual network with Azure Synapse consider the implications this will have on Apache Spark in Azure Synapse. One such consideration is the inability to use Spark SQL when accessing data.\nPerformance Efficiency\nFile size and File type: The size of the file and the type of file for reading and writing to and from Apache Spark to cloud storage is key to good performance. Both should be handled prior to ingestion to Apache Spark. Design the architecture to ensure that the files types are file types that is conducive to native ingestion with Apache Spark. Also ensure that the size of the files are large files and not many small files.\nPartitioning: Determine if partitioning at the folder and/or file level will be implemented for your workload. Folder partition limits the amount of data to be searched and read. File partitioning reduces the amount of data to be searched in the file but only applies to specific file formats and needs to be considered in the initial architecture.\nSecurity\nSecurity for the data to be accessed must be considered at the ADLS account that is attached to the Synapse workspace. In addition, determine the security levels that should be required to access any data that is not within the Synapse environment.\n"},{"id":7,"href":"/Synapse-Success-By-Design/docs/project/evaluate-project-team/","title":"Evaluate Project Team","section":"Docs","content":"Sample https://docs.microsoft.com/en-us/learn/modules/explore-roles-responsibilities-world-of-data/\nCertifications https://docs.microsoft.com/en-us/learn/certifications/browse/?products=azure\nEvaluate Project Team Readiness\nWhat are the roles and skillsets required for a successful implementation using Azure Synapse Analytics?\n  Synapse Administrator\n  Synapse Database Administrator\n  Synapse Data Engineer\n  Synapse Data Analyst\n  Synapse Data Scientist\n  Azure Administrator\n  Azure Security Engineer\n  Azure DevOps Engineer\n  Synapse Administrator\nA Synapse Administrator is responsible for the administration of the overall Azure Synapse environment. They’re responsible for the availability and scale of Resource group, Data Lake administration, Analytics Pools, Workspace administration and monitoring.\nThe Azure Synapse Administrator will work closely with all roles to ensure Synapse access, Analytics services, and scale.\nA Synapse Administrator is recommended to complete the Administrator and Cloud Solution Architect\nSynapse Database Administrator\nA Synapse database administrator is responsible for the design, implementation, maintenance, and operational aspects of the Synapse Analytics Pools (Spark, Serverless and Dedicated). They’re responsible for the overall availability and consistent performance and optimizations for the Synapse Pools. They work with stakeholders to implement policies, tools, and processes for backup and recovery plans to recover following a natural disaster or human-made error.\nThe database administrator is also responsible for managing the security of the data in the database, granting privileges over the data, granting, or denying access to users as appropriate.\nSynapse Data Engineer\nA data engineer collaborates with stakeholders to design and implement data-related assets that include data ingestion pipelines, cleansing and transformation activities, and data stores for analytical workloads. They use a wide range of data platform technologies, including relational and nonrelational databases, file stores, and data streams.\nThey\u0026rsquo;re also responsible for ensuring that the privacy of data is maintained within the cloud and spanning from on-premises to the cloud data stores. They also own the management and monitoring of data stores and data pipelines to ensure that data loads perform as expected.\nA Synapse Data Engineer is recommended to complete the Data Engineer certification.\nSynapse Data Analyst\nA data analyst enables businesses to maximize the value of their data assets. They\u0026rsquo;re responsible for designing and building scalable models, cleaning and transforming data, and enabling advanced analytics capabilities through reports and visualizations.\nA data analyst processes raw data into relevant insights based on identified business requirements to deliver relevant insights.\nA Synapse Data Analyst is recommended to complete the DA-100: Analyzing Data with Microsoft Power BI exam\nSynapse Data Scientist\nA Synapse Data Scientist is recommended to complete the Data Scientist and AI Engineer Certifications\nAzure Administrator\nAn Azure Administrator is recommended to complete the Azure Administrator Associate certification.\nAzure Security Engineer\nAn Azure Administrator is responsible for managing Cloud services that span computes, networking, storage, security, and other Cloud capabilities within Microsoft Azure.\nAn Azure Security Engineer is recommended to complete the Security Engineer Associate certification.\nAzure DevOps Engineer\nAn Azure DevOps Engineer is recommended to complete the DevOps Engineer certification.\n"},{"id":8,"href":"/Synapse-Success-By-Design/docs/project/evaluate-project-plan/","title":"Evaluate Project Plan","section":"Docs","content":"In the project’s lifecycle, the most important and extensive planning is performed before implementation. This article will provide a high level QUICK review of the project plan to make sure it contains critical artifacts and information for a successful project.\nBelow is the checklist of items that must be defined and approved before the project is initiated.\nThe Project Plan\n  Core Resource Team – core group of key people, each of whom offers expertise crucial to the project.\n  Scope - this document how the project scope will be defined, verified, measured, and how the work breakdown will be created and defined.\n  Schedule – this defines the time duration required to complete the project.\n  Cost – costs for resources, internal and external, infrastructure/hardware/software\n   As the work breakdown is defined and assigned, we also wanted to make sure we have the following artifacts defined:\n  Migration plan – the plan to migrate from your current legacy system to Synapse.\n  Success Criteria – the critical success criteria for stakeholders/project sponsor, go/no go criteria.\n  Quality Assurance – defining code reviews, development/staging/production promotion approval process.\n  Test plan – defining test cases, success criteria for unit, integration, user testing and metrics to validate the deliverables.\n  Please see the following document for more detail information on Evaluation of Project Plan for the specific area:\n  Evaluate Project Plan Workspace\n  Evaluate Project Plan   Data Integration   Evaluate Project Plan dedicated SQL Pool   Evaluate Project Plan serverless SQL Pool   Evaluate Project Plan Spark Pool   Evaluate Project Team Readiness\n  "},{"id":9,"href":"/Synapse-Success-By-Design/docs/tools-utilities/","title":"Tools Utilities","section":"Docs","content":"Tools #  "},{"id":10,"href":"/Synapse-Success-By-Design/pools/spark-pools/","title":"Spark Pools","section":"Pools","content":"SBD – Spark Pool Design Evaluation #  Apache Spark in Synapse brings the Apache Spark parallel data processing to the Azure Synapse.\nFit Gap Analysis #  When planning to implement Spark pools with Azure Synapse, you first need to ensure Apache Spark Pools in Azure Synapse is the best fit for your workload. These are the items to consider for utilizing Apache Spark in Synapse:\n  Does your workload require data engineering/data preparation?\n  Apache Spark works best for workloads that require:\n  Data Cleaning\n  Transforming semi-structured data like XML into relational\n  Complex free-text transformation (fuzzy matching, NLP)\n  Prepping data for machine\n      Is your workload for data engineering/data preparation complex or simple transformations? Are you looking for a low code/no code approach?\n  For simple transformations such as dropping of columns, casting of columns, or joining two datasets. Consider using Data Flows within Synapse Pipelines.\n  Data Flows also provides a low code/no code approach to your data processing.\n    Does your workload require machine learning on Big Data?\n Apache Spark works well for large datasets that will be used for machine learning. If using small datasets, consider using Azure Machine Learning as the compute.    Do you plan to perform data exploration and need an ad-hoc query analysis on Big Data?\n Apache Spark in Azure Synapse provides Python/Scala/SQL/.NET based data exploration. However, if you need full T-SQL experience consider using Synapse SQL Serverless.    Do you have a current Spark/Hadoop workload and need a unified Big Data Platform?\n  Azure Synapse provides a full unified analytical platform for working with Big Data from Spark to SQL Serverless for ad-hoc queries to SQL on dedicated pools for reporting and serving of data.\n  Moving from a Spark/Hadoop workload from on-prem or another cloud environment may require refactoring and should be considered.\n  If you are looking for a lift and shift approach for your Apache Big Data environment from on-prem to the cloud and require strict data engineering SLA, use HDInsight\n    Architecture Considerations #  To ensure that your Apache Spark pool meets your operational, performance, and security requirements, there are key areas to address in your architecture.\nOperational Excellence #  Environment Configuration\nWhen configuring your environment ensure that you have designed your pool to take advantage of features such as auto-scale and dynamic allocation. Also consider utilizing the automatic pause feature to reduce costs.\nPackage Management\nDetermine if the Apache Spark libraries to be used will be used at a Workspace, Pool, or Session level. When managing packages at each level there are different considerations to consider.\nMonitoring\nApache Spark in Azure Synapse provides built-in Spark pool and application monitoring with the creation of each spark session. Also consider implementing application monitoring with Azure Log Analytics or Prometheus and Grafana to visualize metrics and logs.\nNetworking\nIf designing to use a managed virtual network with Azure Synapse consider the implications this will have on Apache Spark in Azure Synapse. One such consideration is the inability to use Spark SQL when accessing data.\nPerformance Efficiency\nFile size and File type: The size of the file and the type of file for reading and writing to and from Apache Spark to cloud storage is key to good performance. Both should be handled prior to ingestion to Apache Spark. Design the architecture to ensure that the files types are file types that is conducive to native ingestion with Apache Spark. Also ensure that the size of the files are large files and not many small files.\nPartitioning: Determine if partitioning at the folder and/or file level will be implemented for your workload. Folder partition limits the amount of data to be searched and read. File partitioning reduces the amount of data to be searched in the file but only applies to specific file formats and needs to be considered in the initial architecture.\nSecurity\nSecurity for the data to be accessed must be considered at the ADLS account that is attached to the Synapse workspace. In addition, determine the security levels that should be required to access any data that is not within the Synapse environment.\n"},{"id":11,"href":"/Synapse-Success-By-Design/pools/provisioned-sql-pools/","title":"Provisioned Sql Pools","section":"Pools","content":"SQL Provisioned Pool #  Azure Synapse Analytics is an analytics service that brings together enterprise data warehousing and Big Data analytics. Dedicated SQL pool (formerly SQL DW) refers to the enterprise data warehousing features that are available in Azure Synapse Analytics.\nSynapse SQL leverages a scale out architecture to distributehugo computational processing of data across multiple nodes. Compute is separate from storage, which enables you to scale compute independently of the data in your system.\nFor dedicated SQL pool, the unit of scale is an abstraction of compute power that is known as a data warehouse unit.\nReview: Dedicated SQL pool architecture - Azure Synapse Analytics | Microsoft Docs\nTo determine if Synapse is a good fit for a migration, we need to understand what is currently being used in the original system and determine if there are Synapse gaps that can be a potential blocker.\nSynapse Assessment #  Base Checklist:\n Identity the Source Workload Type (DW/OLTP)?    Identity the Source ETL/ELT Tool\n  Does it support Synapse?\n  How much work is needed to move from Source to Synapse?\n      Identify the Data Sources\n  Identify the Data Ingestion processes\n  Ad-Hoc?\n  Batch\n  Files\n  Stream\n  Etc.\n      Data Consumption Platform\n  Does the current Tool support Synapse?\n  Type of access (DQ/Import)\n  Max users / Concurrent users\n      Synapse Sizing (How do choose Synapse DWU)\n  Identify Source Number of Nodes/Memory/CPUs\n  Source Data Volume\n  Source Concurrency\n  Determine “Active Data Set”\n      Feature Gaps\n  Identify unsupported data types (ex. Xml, arrays, spatial).\n  Identify unsupported features (ex: Cursors)\n  Are there gaps without a work around?\n    @Tammy\nWe have an internal chart with the Synapse sizing recommendations, but I don’t know if it’s public or not.\nDefine Target Architecture #  Add url for MDW Architectures.\nReview: Azure Data Architecture Guide - Azure Architecture Center | Microsoft Docs\nReuse and Re-Write?\nThe purpose of the following is to try to reduce blockers and avoid some issues related to Synapse SQL Serverless pools frequently found on the course of the implementation of modern data architectures based on Azure Synapse or later in the production stages. To be consistent with guiding tenets of Well Architected Framework (Microsoft Azure Well-Architected Framework - Azure Architecture Center | Microsoft Docs) that aim to improve the quality of a workload we are going to try to reuse the main pillars that will allow better structure of the information and also help to introduce necessary checkpoints in the corresponding phases of you project execution.\nMigration #  Review: Azure Synapse Analytics: Migration guide - Azure Synapse Analytics | Microsoft Docs\nProject Team #  Topic: The project must have the right number of people with the right skillset (Synapse is not SQL)\nBest Practices and recommendations #  Please review our best practices from official documentation\n  Best practices for dedicated SQL pools - Azure Synapse Analytics | Microsoft Docs\n  Instead of ETL, design ELT - Azure Synapse Analytics | Microsoft Docs\n  Data loading best practices - Azure Synapse Analytics | Microsoft Docs\n  Development best practices for Synapse SQL - Azure Synapse Analytics | Microsoft Docs\n  Performance tuning with result set caching - Azure Synapse Analytics | Microsoft Docs\n  Designing tables - Azure Synapse Analytics | Microsoft Docs\n  Using T-SQL loops - Azure Synapse Analytics | Microsoft Docs\n  Using stored procedures - Azure Synapse Analytics | Microsoft Docs\n  Optimizing transactions - Azure Synapse Analytics | Microsoft Docs\n  Workload management - Azure Synapse Analytics | Microsoft Docs\n  Manageability and monitoring - overview - Azure Synapse Analytics | Microsoft Docs\n  Build integrated solutions - Azure Synapse Analytics | Microsoft Docs\n  Scope and work sprints #  Make sure that you define the scope of the work, aligned with a project plan with deliverables and milestones, to avoid unwanted surprises during the delivery.\nEngage a project manager into the team to assure that all work is correctly tracked, and that you are not getting into delays.\nTesting methodology #  Success Criteria #  "},{"id":12,"href":"/Synapse-Success-By-Design/project/evaluate-project-team/","title":"Evaluate Project Team","section":"Projects","content":"Sample https://docs.microsoft.com/en-us/learn/modules/explore-roles-responsibilities-world-of-data/\nCertifications https://docs.microsoft.com/en-us/learn/certifications/browse/?products=azure\nEvaluate Project Team Readiness\nWhat are the roles and skillsets required for a successful implementation using Azure Synapse Analytics?\n  Synapse Administrator\n  Synapse Database Administrator\n  Synapse Data Engineer\n  Synapse Data Analyst\n  Synapse Data Scientist\n  Azure Administrator\n  Azure Security Engineer\n  Azure DevOps Engineer\n  Synapse Administrator\nA Synapse Administrator is responsible for the administration of the overall Azure Synapse environment. They’re responsible for the availability and scale of Resource group, Data Lake administration, Analytics Pools, Workspace administration and monitoring.\nThe Azure Synapse Administrator will work closely with all roles to ensure Synapse access, Analytics services, and scale.\nA Synapse Administrator is recommended to complete the Administrator and Cloud Solution Architect\nSynapse Database Administrator\nA Synapse database administrator is responsible for the design, implementation, maintenance, and operational aspects of the Synapse Analytics Pools (Spark, Serverless and Dedicated). They’re responsible for the overall availability and consistent performance and optimizations for the Synapse Pools. They work with stakeholders to implement policies, tools, and processes for backup and recovery plans to recover following a natural disaster or human-made error.\nThe database administrator is also responsible for managing the security of the data in the database, granting privileges over the data, granting, or denying access to users as appropriate.\nSynapse Data Engineer\nA data engineer collaborates with stakeholders to design and implement data-related assets that include data ingestion pipelines, cleansing and transformation activities, and data stores for analytical workloads. They use a wide range of data platform technologies, including relational and nonrelational databases, file stores, and data streams.\nThey\u0026rsquo;re also responsible for ensuring that the privacy of data is maintained within the cloud and spanning from on-premises to the cloud data stores. They also own the management and monitoring of data stores and data pipelines to ensure that data loads perform as expected.\nA Synapse Data Engineer is recommended to complete the Data Engineer certification.\nSynapse Data Analyst\nA data analyst enables businesses to maximize the value of their data assets. They\u0026rsquo;re responsible for designing and building scalable models, cleaning and transforming data, and enabling advanced analytics capabilities through reports and visualizations.\nA data analyst processes raw data into relevant insights based on identified business requirements to deliver relevant insights.\nA Synapse Data Analyst is recommended to complete the DA-100: Analyzing Data with Microsoft Power BI exam\nSynapse Data Scientist\nA Synapse Data Scientist is recommended to complete the Data Scientist and AI Engineer Certifications\nAzure Administrator\nAn Azure Administrator is recommended to complete the Azure Administrator Associate certification.\nAzure Security Engineer\nAn Azure Administrator is responsible for managing Cloud services that span computes, networking, storage, security, and other Cloud capabilities within Microsoft Azure.\nAn Azure Security Engineer is recommended to complete the Security Engineer Associate certification.\nAzure DevOps Engineer\nAn Azure DevOps Engineer is recommended to complete the DevOps Engineer certification.\n"},{"id":13,"href":"/Synapse-Success-By-Design/project/evaluate-project-plan/","title":"Evaluate Project Plan","section":"Projects","content":"In the project’s lifecycle, the most important and extensive planning is performed before implementation. This article will provide a high level QUICK review of the project plan to make sure it contains critical artifacts and information for a successful project.\nBelow is the checklist of items that must be defined and approved before the project is initiated.\nThe Project Plan\n  Core Resource Team – core group of key people, each of whom offers expertise crucial to the project.\n  Scope - this document how the project scope will be defined, verified, measured, and how the work breakdown will be created and defined.\n  Schedule – this defines the time duration required to complete the project.\n  Cost – costs for resources, internal and external, infrastructure/hardware/software\n   As the work breakdown is defined and assigned, we also wanted to make sure we have the following artifacts defined:\n  Migration plan – the plan to migrate from your current legacy system to Synapse.\n  Success Criteria – the critical success criteria for stakeholders/project sponsor, go/no go criteria.\n  Quality Assurance – defining code reviews, development/staging/production promotion approval process.\n  Test plan – defining test cases, success criteria for unit, integration, user testing and metrics to validate the deliverables.\n  Please see the following document for more detail information on Evaluation of Project Plan for the specific area:\n  Evaluate Project Plan Workspace\n  Evaluate Project Plan   Data Integration   Evaluate Project Plan dedicated SQL Pool   Evaluate Project Plan serverless SQL Pool   Evaluate Project Plan Spark Pool   Evaluate Project Team Readiness\n  "}]